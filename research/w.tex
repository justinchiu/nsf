\documentclass[12pt]{article}

\usepackage[letterpaper,margin=0.75in]{geometry}
\usepackage[T1]{fontenc}
\usepackage{newtxtext}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\paragraph{Keywords}
information networks, natural language processing, information extraction, latent variable models

\paragraph{Relevance to Army BAA: II. A. c. iii. (3) Information Networks}
In order to provide decision makers with the information
necessary to make informed choices as well as predict the effects of those choices,
we must have an efficient representation of relevant information and a predictive model
for the resultant state of the world given a choice.
Information networks provide a graphical representation of information and how it
propagates through a network.
We focus on \textit{knowledge graphs}, information networks where each node contains a set of facts
about an entity and an edge describes how the facts in one node influence the facts in another.
Knowledge graphs provide an intuitive and queryable representation of knowledge.
A decision maker may query the relevant nodes to gain situational awareness, and
when simulating a decision that alters the information in one node
the graph can easily propagate those changes by virtue of its representation.

Given that a knowledge graph must represent an extremely large number of facts and relationships,
it is infeasible to specify these completely by hand.
Many recent works have focused on learning to fill in the missing edges of a knowledge graph
by recognizing patterns in fully-labeled subgraphs in order to predict whether there should
be an edge between two nodes in an unlabeled subgraph \citep{chen2018diva}.
By filling in missing edges, we are able to use the relationships specified by the edges
in order to reason about the facts contained in a node conditioned on the facts of its
neighbours.
For example, given two nodes corresponding to two different entities,
we can leverage the relationships between the two nodes to infer the
values of one given the other.

However, in addition to supervision over subgraphs, 
we generally also have access to large amounts of unlabeled and unstructured data
in the form of natural language text.
We propose an approach that is orthogonal to edge completion,
which instead relies on jointly modeling
text and the nodes of the information network.
We aim to leverage natural language text by
learning an information extraction system that is able to 
extract facts from text to fill in the nodes of a knowledge graph
given \textbf{only} the assumption that the text was generated from the information
represented in the knowledge graph.
Although this is a strong assumption, we believe it is a reasonable starting point.
We note that information extraction is complementary to edge completion as a method
for inferring missing values in the nodes of a knowledge graph,
and although we focus on information extraction due to its ability to leverage text, 
the two approaches can be combined in future work.

In this proposal we present a method towards automating the
training of information extraction systems with unlabeled corpora
by recasting the information extraction problem as a conditional generative modeling problem.
Specifically, we plan to use the performance of a deep generative model of text 
as signal for learning an extraction system.

\begin{comment}
(Include exemplar knowledge graph diagram, and show that we focus on filling nodes paired with text)
(Diagram highlighting the difference between modeling edges without text for KG completion
vs only modeling nodes jointly with text. Highlight complementary nature and emphasize
combination as future work)
\end{comment}

\paragraph{Introduction}
% Information extraction
The goal of information extraction is to produce structured representations of information
given unstructured text.
We use summaries of basketball games as an example to ground the concept of an
information extraction system.
In the context of a basketball game summary, an information extraction system
would infer all the statistics associated with a player given the summary.
A typical approach to an information extraction system is the following pipeline,
where each stage has a model that is trained independently from the other stages:
first segment the text into entities and values,
then extract named entities and possibly perform
coreference resolution by predicting whether segments refer to the same entity, 
and finally relation extraction where we identify the relationships between the extracted
entities and values.
We can then use the extracted relationships along with the associated entities and values
to populate the nodes of a knowledge graph, where each node would
correspond to a player and contain their associated statistics.
See Figure 1 for an example of this process.
This process is orthogonal and complementary to using existing nodes in a knowledge graph
to fill in missing ones.
As we are interested in utilizing large amounts of unlabeled corpora for 
the training of an information extraction system, we restrict our focus
to learning the values of each node independently given the text.
Neural networks, in combination with latent variable models (LVMs),
provide a method for training such an information extraction pipeline end-to-end.

The progression from a pipelined system into one that is trained jointly
has precedent in the field of natural language processing: machine translation.
Previously, statistical machine translation utilized a highly pipelined approach,
where each stage utilized a model that was trained independently of the others.
The approach was unified with an end-to-end neural model in \citet{bahdanau2014mt}.
We recently recast \citet{bahdanau2014mt}'s model in the LVM framework in
\citet{deng2018attn} which resulted in performance gains as well as improved
sample complexity.
Our proposal is inspired by the improvements seen in machine translation:
we wish to specify an information
extraction system that is learned end-to-end rather than stage by stage,
and we use the LVM framework to do so.
LVMs provide a principled way of specifying semi-supervised models \citep{kingma2014ssvae},
and have been shown to have better sample complexity than discriminative models
\citep{deng2018attn,ng2001discgen}.

Although LVMs have recently been proposed in the context of knowledge graph
completion \citep{chen2018diva,qu2017ssre}, they either do not utilize text
or do not have a (conditional) generative model of text.
We argue that the generative model we specify should be as close to 
the data generating process as possible.
In particular, \citet{chen2018diva} formalize predicting the relationship between two nodes
as a LVM but do not utilize text at all.
\citet{qu2017ssre} also learn a LVM for the relationships between nodes,
which includes using the text to learn a distributed representations of entities.
However, their approach does not take into account the generative process of the text,
which limits the expressibility of their model.
We propose to explicitly model how a text is created given a knowledge graph
using a conditional generative model with latent variables.

In this proposal, we focus on the class of LVMs known as hidden semi-Markov models (HSMMs),
used in \citet{liang2009semalign} as a conditional generative model for the
task of aligning segments of text to
nodes in a knowledge graph without supervision.
As in \citet{liang2009semalign}, we are interested in learning a conditional generative model of text so that
we can minimize the amount of supervision necessary for training an information extraction system.
The performance of their conditional generative model of text provides signal for learning the alignments:
if the likelihood of a segment of text improves when moving from a
past alignment choice to a new one,
then the new alignment is more likely to be correct given a suitably strong likelihood model.
We use that same intuition to formulate a related LVM for a semi-supervised information extraction
which aims to model not just the alignments from segments of text to nodes in a knowledge graph
but also the values in the nodes themselves.
By parameterizing the HSMM with neural networks as in \citet{wiseman2018template},
we hope to incorporate recent progress in parameterizing LVMs with neural networks
so as to learn a more accurate information extraction system by using a more powerful
generative model.

\paragraph{Background}
We consider datasets consisting of aligned data and text
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single datum and text as $\br,\by$, omitting the superscript.
Each datum $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, where each record $r_i = (e_i, t_i, v_i)$
is a tuple containing an entity, type, and value.
The datum $\br$ is a knowledge base, or equivalently an information network
without a representation of the causal effects between records.
We refer collectively to all the entities, types, and values in a given datum $\br$ as
$\be,\bt,\bv$ respectively.
Each text $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens each from a vocabulary $V$.

We proceed to detail how to specify a LVM,
then provide a concrete example linking the above dataset description to our LVM formulation:
Let variables $\bz$ be unobserved or latent, $\by$ observed, and $\bx$ taken as conditioning
and thus not modelled.
For information extraction we are interested in distributions
that can be specified as $p(\bz\mid\by,\bx)$,
where $\bz$ and $\bx$ may correspond to various quantities depending on the task
but $\by$ is always the text.

As a concrete example, we use the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games $\by$ aligned with the respective
box scores $\br$ of those games.
Consider a datum that consists of a single record,
$\br = \set{(e_1 = \textrm{Jeremy\_Lin}, t_1 = \textrm{POINTS}, v_1 = 19)}$,
and a simple statement $\by = $``Jeremy Lin scored 19 points''.
In its simplest incarnation, the process of information extraction may be to infer any
subset of $\br$, which in this case will be our latent $\bz$, given the remaining elements in $\br$
which corresponds to $\bx$, as well as the text $\by$.
For example, we may want to infer the value $v_1$ given 
the entity Jeremy Lin, the type POINTS, as well as the text $\by$.
In this case, we would have $\bz = \set{v_1}$ and $\bx = \set{e_1,t_1}$.
In an alternative task, we may want to infer the value $v_1$ 
as well as the type $t_1$ given $\by$ and $e_1$, therefore $\bz = \set{v_1,t_1}$ and $\bx=\set{e_1}$.
%(Will switch to ACE dataset after I figure out what's going on with the data.)

\begin{comment}
%(clarify)
Note that we are not constrained to setting $\bz$ to subsets of $\br$.
We also consider the case where $\bz$ includes alignments from individual words $y_t$
to records $r_i$. We denote the alignments $\ba = \set{a_1,\ldots,a_T}$,
where each $a_t$ is associated with $y_t$ and selects a record $r_i$ such that $a_t = i$.
%(NEED TO FIX ALIGNMENTS AND T vs t, HMM vs HSMM)
\end{comment}

\paragraph{Proposal}
We propose to verify the efficacy of the LVM framework in the
semi-supervised information extraction setting,
with the goal of demonstrating strong extractive performance with minimal labels.
We present one instance of a LVM and outline how it can be used to obtain
an information extraction model without direct supervision,
then argue that the same approach can be applied in even more ambitious settings.
Our proposed LVM is a conditional generative model that specifies
the relationship between data, specifically the entities and types, and text.
We denote this model \texttt{Values}.
%Let $\by$ be the text, $\ba$ be a latent variable that represents the
%alignments from words to records,
%$\bv$ all the values in a datum of records,
%and $\be,\bt$ the entities and types respectively.
Similar to the models defined in \citet{wiseman2018template} and \citet{liang2009semalign},
our model takes the form of a hidden semi-Markov model (HSMM).
The primary difference is that the other models simply assumed the records
were complete and conditioned on them, whereas ours learns to generate the values.
For our model, we have observed $\bx = \set{\be,\bt}$ and latent $\bz = \set{\bv,\ba}$, 
where $\be$ are the entities, $\bt$ are the types, $\bv$ are the values, and $\ba$ are
record indices.
\texttt{Values} is given by the following generative process:
\begin{enumerate}
\item Prior Value Choice: $p(\bv\mid\be,\bt)=p(\bv\mid\bx)$.
For each entity and type pair in our datum of records, we predict a value.
For example, given `Jeremy\_Lin' and POINTS, we predict 19.
Each value is predicted independently.
As both the entites and types are observed, we have $\bx=\set{\be,\bt}$.
\item Prior Record Choice: $p(\ba\mid\bv,\bx) = \prod_i p(a_i\mid a_{i-1},\bv,\bx)$.
Conditioned on our choices of values as well as the given entities and records,
in other words a completed data with no missing values,
we choose a sequence of records $\ba = \set{a_1,\ldots,a_I}$ to describe.
\item Word Choice: $p(\by\mid\ba,\bv,\bx) = p(\by\mid\bz,\bx)$.
For each chosen record $a_i$,
we choose a sequence of words $\by_i = \set{y_{i1},\ldots,y_{iJ}}$ to describe the record.
With the HSMM formulation, we have that within each segment aligned to a record,
the words are modeled autoregressively.
This distribution is the likelihood of the text given the latent variables $\bz$,
the record choices and value choices.
\end{enumerate}
We obtain an information extraction by using the 
\textbf{posterior} distribution over alignments and values:
\begin{linenomath*}
$$
p(\bz\mid\by,\bx)=\frac{p(\by,\bz\mid\bx)}{p(\by\mid\bx)}=\frac{p(\by,\bz\mid\bx)}{\sum_\bz p(\by,\bz\mid\bx)}.
$$
\end{linenomath*}
Although the HSMM formulation allows the summation (marginalization) over alignments to be carried out efficiently,
we cannot marginalize over value assignments.
We instead resort to variational inference as in \citet{deng2018attn},
where we learn an approximation of the posterior distribution $q(\bz\mid\by,\bx)$
with a separate model.
% Can marginalize over alignments, so only a partial variational approximation is needed
We can train the conditional generative model and the approximate posterior jointly 
by maximizing a lower bound on the log marginal likelihood or evidence of $\by$,
called the evidence lower bound (ELBO) with gradient-based methods.
The resulting approximate posterior $q(\bz\mid\by,\bx)$ can be used independently of the 
generative model as an information extraction system that gives a distribution over
values in a table of records and alignments from text to records.

We will evaluate our approach on the 
TAC KBP 2015 slot filling, TACRED \citep{zhang2017slotfilling},
and ROTOWIRE \citep{wiseman2017d2t} datasets in order to compare to previous work.
The goal will be to demonstrate competitive performance on
extraction metrics such as precision, recall, and F1, while using as little supervision
as possible by ignoring subsets of the given information.
For example, with the model \texttt{Values}, we can allow the model to only learn from
subsets of the given values (or none at all) in ROTOWIRE, such as only
the home team's statistics, at training time.
Given success in that goal, the next step would be to extend the model
with more structure with the aim of boosting sample and label efficiency.
Possible extensions in this direction include parameterizing entities including
entity tracking and coreference resolution \citep{haghighi2010coref};
learning the types of relations in a semi-supervised manner;
leveraging discourse structure to improve the conditional generative model
\citep{sauper2009wiki}; explicitly modeling nuisance variables such as
author style \citep{hsu2017speech}; and incorporating multi-hop
reasoning in order to leverage relationships between entities
\citep{chen2018diva,rock17prove}.

\begin{comment}
%TODO: Experiments, Evaluation, and Expectations? Also shortcomings? Short paragraph
We will evaluate our initial approach on the Rotowire dataset, and
extensions to our model that will include entity tracking and event resolution on the 
automatic content extraction (ACE) \citep{ace2004} and the Text Analysis Conference's
Streaming Multimedia Knowledge Base Population (SM-KBP) datasets.
We expect the variance of the gradient estimator to be an issue, in particular its
effect on sample complexity. In previous work, we observed that gradient estimators
based on exact inference resulted in better sample complexity than
approximate inference \citep{deng2018attn}, and we expect that controlling the variance 
of the gradient estimator will be of paramount importance for the success of our proposed method.
\end{comment}

Given admittance to the NDSEG Fellowship Program, I will evaluate the application of
LVMs to the problem of information extraction.
As a result of the digital age, the ubiquity of information networks as well as their 
enormous growth makes it clear that a method for training information extraction systems
with minimal supervision is a necessity.
I will push for scalable information extraction systems that require minimal supervision
by recasting information extraction as a generative modeling problem.

%\begin{comment}
\newpage
\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item Importance of representation and correctness of knowledge in decision making,
            as well as the power of prediction.
        \item Information networks are one way of representing knowledge as a graph
            as well as interactions?
        \item What are information networks?
            Graphical representation of objects, their characteristics of interest,
            and their relationship to objects.
        \item Recent work focuses on learning structure between nodes
            in order to fill parts of the graph that may be missing \cite{chen2018diva}.
        \item We focus on the orthogonal approach of completing knowledge graphs using
            large amounts of unstructured text rather than the relationships between nodes.
        \item Reduce information networks to knowledge bases by removing edges
            (kind of interesting as we can view knowledge bases as a mean field approximation
            to information networks where info networks must specify not only
            conditional distributions but also interventional distributions).
        \item In this proposal we present a framework towards automating the
            training of text-centric information extraction systems with minimal supervision.
        \item Shine some hope on interventional distribution representations?
            (This might be a little difficult)
        \end{enumerate}
    \item Supervision in Information Extraction
        \begin{enumerate}
        \item Define information extraction
            \begin{enumerate}
            \item The goal is to produce structured representations of information from
                a given unstructured text.
            \item Ideally, but not necessarily computer-readable in addition to human-readable.
            \item A typical pipeline for information extraction includes
                text segmentation, named entity recognition, coreference resolution,
                relation extraction, and finally producing structured representations of the unlabeled text.
            \item This is knowledge-base completion.
            \item We are primarily interested in knowledge-base completion,
                as the other tasks such named entity recognition are typically
                part of a pipeline aimed at knowledge-base completion.
            \end{enumerate}
        \item Argue for LVM approach to unify all parts of the pipeline and train
            end-to-end with minimal supervision.
        \item We aim to unify these two approaches through recently developed techniques for training LVMs
            parameterized with neural networks.
        \end{enumerate}
    \item End-to-end trends and LVMs
        \begin{enumerate}
        \item Draw parallels to progress in translation, mainly in terms of structure?
            \begin{enumerate}
            \item Statistical machine translation: pipelines
            \item End-to-end with Sutskever
            \item Deterministic structure via attention Bahdanau
            \item Latent attention \citet{deng2018attn}
            \end{enumerate}
        \item Leverage modularity of both the neural network and LVM frameworks
            to incorporate more of the pipeline into a joint training framework.
        \item \citet{qu2017ssre} Semi-supervised relation extraction (which is pretty much similar)
            How is what I want different? Capitalize on the progress of powerful generative models,
            can argue that their model is much weaker in terms of generative power,
            but the framework is similar (they perform coordinate ascent in a LVM).
        \end{enumerate}
    \item Recent advances in neural LVMs
        \begin{enumerate}
        \item Semi-supervised LVMs \citet{kingma2014ssvae}?
        \item Demonstrate that parameterization with a neural network does not affect computational
            complexity of inference.
        \item Then the same technique can be applied to model with more structure,
            as long as the graphical model itself permits tractable inference.
        \item In this proposal, we focus on the hidden semi-Markov model (HSMM),
            used in \citet{liang2009semalign} for the task of aligning segments of text to
            records in a knowledge base without supervision. 
        \item As in \citet{liang2009semalign}, we are interested in learning a generative model of text so that
            we can minimize the amount of supervision necessary for training an
            information extraction system.
        \item Also that although worse sample complexity, using an approximate posterior
            with monte carlo sampling achieves comparable performance.
        \end{enumerate}
    \end{enumerate}
\item Background and Notation
    \begin{enumerate}
    \item Formal notation for elements of the dataset
    \item Define the distribution we would like to learn: $p(z\mid y, x)$.
        $z$ and $x$ are placeholders and will change, but $y$ is always the text.
    \item Link to rotowire example
        (argument is that ACE is made up of ontonotes-like sentences, so all short-form)
    \item Clarify that the scope of posterior inference is very general.
    \end{enumerate}
\item Proposal
    \begin{enumerate}
    \item Outline approach
        \begin{enumerate}
        \item Choose a subset of available data as conditioning,
            and thus it is not modelled.
        \item The joint distribution of the remaining variables,
            both observed and unobserved, will be modelled.
        \end{enumerate}
    \item Link back to motivation. We want to scale information extraction
        by requiring less supervision.
    \item We present one model as an example, which we will serve as
        a starting point for the proposed research.
    \item Define generative model: HSMM as in \citep{liang2009semalign},
        and \citep{wiseman2018template}.
        The generative story (a picture would be helpful):
        \begin{enumerate}
        \item Fill in values
        \item Choose alignments
        \item Choose words
        \end{enumerate}
    \item Define IE as the distribution we would like to learn
        \begin{enumerate}
        %\item Align: $p(c\mid\by,\br)$
        \item Values: $p(c,\bv\mid\by,\be,\bt)$ (Just this one)
        %\item ??: $p()$
        \end{enumerate}
    \item We either use the posterior distribution of the conditional model
        or learn an approximation of it.
    \item Training and Inference
        \begin{enumerate}
        \item As we are dealing with documents of significant length,
            we train with an approximate posterior in order to satisfy memory constraints.
        \item Highlight that the approx posterior is a SEPARATE model
            that can be used completely independently from generative model,
            i.e. we throw away generative model after training.
        \item We maximize a lower bound on the log marginal likelihood,
            called the evidence lower bound.
        \end{enumerate}
    \item Experiments, evaluation, and expectation
        \begin{enumerate}
        \item Evaluate on Rotowire. Highlight long-form text?
        \item We evaluate \texttt{Values} using the precision, recall, and F1 score on the task
            of predicting the values associated with entities, otherwise known as slot-filling.
        \item What would success look like?
            \begin{enumerate}
            \item Competitive to supervised methods when supervision is available
            \item But able to be applied when supervision is not available
            \item Able to leverage lots of unlabeled data during training,
                and success would see a marked improvement over purely supervised methods
                as well as the purely supervised version of this model.
            \item Would provide explanations for the answers (ie segmentations).
            \end{enumerate}
        \item Also on ACE?
        \end{enumerate}
    \item Future work
        \begin{enumerate}
        \item Incorporate more structure into the generative model,
            for example entity tracking or coreference resolution \citet{haghighi2010coref}.
        \item Model more structure in the data, for example the edges between nodes
            in the knowledge graph \citet{chen2018diva}.
        \item `Multi-hop' reasoning, where we try to compose relationships to infer new ones,
            i.e. through unification \citet{chen2018diva,rock17prove}.
        \end{enumerate}
    \item Conclusion
        \begin{enumerate}
        \item Please accept!
        \item Recap: Minimal supervision IE systems so that they can scale to
            extracting information for large information networks from large bodies of text.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
%\end{comment}

\newpage
\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

