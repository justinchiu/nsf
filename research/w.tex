\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\paragraph{Keywords}
information networks, natural language processing, information extraction, latent variable models

\paragraph{Relevance to Army BAA: II. A. c. iii. (3) Information Networks}
In order to provide decision makers with the information
necessary to make informed choices as well as predict the effects of those choices,
we must have an efficient representation of relevant information and a predictive model
for the resultant state of the world given a choice.
Information networks provide a graphical representation of information and how it
propagates through a network.
We focus on \textit{knowledge graphs}, information networks where each node contains a set of facts
about an entity and an edge describes how the facts in one node influence the facts in another.
Knowledge graphs provide an intuitive and queryable representation of knowledge.
A decision maker may query the relevant nodes to gain situational awareness, and
when simulating a decision that alters the information in one node
the graph can easily propagate those changes by virtue of its representation.

Given that a knowledge graph must represent an extremely large number of facts and relationships,
it is infeasible to specify these completely by hand.
Many recent works have focused on learning to fill in the missing edges of a knowledge graph
by recognizing patterns in fully-labeled subgraphs in order to predict whether there should
be an edge between two nodes in an unlabeled subgraph \citep{chen2018diva}.
By filling in missing edges, we are able to use the relationships specified by the edges
in order to reason about the facts contained in a node conditioned on the facts of its
neighbours.

We propose an orthogonal approach which instead relies on jointly modeling
text as well as the nodes of the information network.
We aim to leverage copious amounts of natural language text by
learning an information extraction system that is able to 
extract facts from text to fill in the nodes of a knowledge graph.
In this proposal we present a method towards automating the
training of information extraction systems with unlabeled corpora
by recasting the information extraction problem as a generative modeling problem.
Specifically, we plan to use the performance of a deep generative model of text 
as signal for learning an extraction system.

(Include exemplar knowledge graph diagram, and show that we focus on filling nodes paired with text)
(Diagram highlighting the difference between modeling edges without text for KG completion
vs only modeling nodes jointly with text. Highlight complementary nature and emphasize
combination as future work)

\paragraph{Introduction}
% Information extraction
The goal of information extraction is to produce structured representations of information
given unstructured text.
We use summaries of basketball games as a running example.
In the context of a basketball game summary, an information extraction system
would infer all the statistics associated with a player given the summary.
A typical approach to an information extraction system is the following pipeline,
where each stage has a model that is trained independently from the other stages:
first segment the text into entities and values,
then extract named entities and possibly perform
coreference resolution by predicting whether segments refer to the same entity, 
and finally relation extraction where we identify the relationships between the extracted
entities and values.
We can then use the extracted relationships along with the associated entities and values
to populate the nodes of a knowledge graph, where each node would
correspond to a player and contain their associated statistics.
See Figure 1 for an example of this process.
This is orthogonal and complementary to using existing nodes in a knowledge graph
to fill in missing ones.
As we are interested in utilizing large amounts of unlabeled corpora for 
the training of an information extraction system, we restrict our focus
to learning the values of each node independently given the text.
Neural networks, in combination with latent variable models (LVMs),
provide a method for training such an information extraction pipeline jointly
and end-to-end.

The progression from a pipelined system into one that is trained jointly
has precedent in the field of natural language processing;
it recently occurred in machine translation.
Previously, statistical machine translation utilized a highly pipelined approach,
where each stage utilized a model that was trained independently of the others.
The approach was unified with an end-to-end neural model in \citet{bahdanau2014mt}.
We recently recast \citet{bahdanau2014mt}'s model in the LVM framework in
\citet{deng2018attn} which resulted in performance gains as well as improved
sample complexity.
This progression parallels our proposal: we wish to specify an information
extraction system that is learned end-to-end rather than in a pipeline,
and we use the LVM framework to do so.
LVMs provide a principled way of specifying either semi-supervised or unsupervised models
\citep{kingma2014ssvae},
and have been shown to have better sample complexity than discriminative models
\citep{deng2018attn,ng2001discgen}.

Although LVMs have recently been proposed in the context of knowledge graph
completion \citep{chen2018diva,qu2017ssre}, they either do not utilize text
or do not have a generative model of text.
We argue that the generative model we specify should be as close to 
the data generating process as possible.
In particular, \citet{chen2018diva} does not utilize text at all
and \citet{qu2017ssre} do not incorporate generative model of the text.
We propose to explicitly model how a text is created given a knowledge graph
using a conditional generative model with latent variables.

In this proposal, we focus on the class of LVMs known as hidden semi-Markov models (HSMMs),
used in \citet{liang2009semalign} as a generative model for the
task of aligning segments of text to
nodes in a knowledge graph without supervision.
As in \citet{liang2009semalign}, we are interested in learning a generative model of text so that
we can minimize the amount of supervision necessary for training an information extraction system.
The performance of their generative model of text provides signal for learning the alignments:
if the likelihood of a segment of text improves when moving from a
past alignment choice to a new one,
then the new alignment is more likely to be correct given a suitably strong likelihood model.
We use that same intuition to formulate a related LVM for weakly supervised information extraction
which aims to model not just the alignments from segments of text to nodes in a knowledge graph
but also the values in the nodes themselves.
By parameterizing the generative HSMM with neural networks as in \citet{wiseman2018template},
we hope to incorporate recent progress in parameterizing LVMs with neural networks
so as to learn a more accurate information extraction system by using a more powerful
generative model -- taking care to ensure that the model can be trained with minimal supervision.
(cite \citet{deng2018attn} for training strategies)

\paragraph{Background}
We consider datasets consisting of aligned data and text
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single datum and text as $\br,\by$, omitting the superscript.
Each datum $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, where each record $r_i = (e_i, t_i, v_i)$
is a tuple containing an entity, type, and value.
The datum $\br$ is a knowledge base, or equivalently an information network
without a representation of the causal effects between records.
We refer collectively to all the entities, types, and values in a given datum $\br$ as
$\be,\bt,\bv$ respectively.
Each text $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens each from a vocabulary $V$.

We proceed to detail how to specify a LVM,
then provide a concrete example linking the above dataset description to our LVM formulation:
Let variables $\bz$ be unobserved or latent, $\by$ observed, and $\bx$ taken as conditioning
and thus not modelled.
For information extraction we are interested in distributions
that can be specified as $p(\bz\mid\by,\bx)$,
where $\bz$ and $\bx$ may correspond to various quantities depending on the task
but $\by$ is always the text.

As a concrete example, we use the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games $\by$ aligned with the respective
box scores $\br$ of those games.
Consider a datum that consists of a single record,
$\br = \set{(e_1 = \textrm{Jeremy\_Lin}, t_1 = \textrm{POINTS}, v_1 = 19)}$,
and a simple statement $\by = $``Jeremy Lin scored 19 points''.
In its simplest incarnation, the process of information extraction may be to infer any
subset of $\br$, which in this case will be our latent $\bz$, given the remaining elements in $\br$
which corresponds to $\bx$, as well as the text $\by$.
For example, we may want to infer the value $v_1$ given 
the entity Jeremy Lin, the type POINTS, as well as the text $\by$.
In this case, we would have $\bz = \set{v_1}$ and $\bx = \set{e_1,t_1}$.
In an alternative task, we may want to infer the value $v_1$ 
as well as the type $t_1$ given $\by$ and $e_1$, therefore $\bz = \set{v_1,t_1}$ and $\bx=\set{e_1}$.
%(Will switch to ACE dataset after I figure out what's going on with the data.)

(clarify)
Note that we are not constrained to setting $\bz$ to subsets of $\br$.
We also consider the case where $\bz$ includes alignments from individual words $y_t$
to records $r_i$. We denote the alignments $\ba = \set{a_1,\ldots,a_T}$,
where each $a_t$ is associated with $y_t$ and selects a record $r_i$ such that $a_t = i$.
(NEED TO FIX ALIGNMENTS AND T vs t, HMM vs HSMM)

\paragraph{Proposal}
We propose to verify the efficacy of the LVM framework in the
weakly supervised information extraction setting,
with the goal of demonstrating strong extractive performance with minimal labels.
We present one instance of a LVM and outline how it can be used to obtain
an information extraction model without direct supervision,
then argue that the same approach can be applied in even more ambitious settings.
Our first LVM is a conditional model that specifies
the relationship between data, specifically the entities and types, and text.
We denote this model \texttt{Values}.
%Let $\by$ be the text, $\ba$ be a latent variable that represents the
%alignments from words to records,
%$\bv$ all the values in a datum of records,
%and $\be,\bt$ the entities and types respectively.
Similar to the models defined in \citet{wiseman2018template} and \citet{liang2009semalign},
our model takes the form of a hidden semi-Markov model (HSMM).
The primary difference is that the other models simply assumed the records
were complete and conditioned on them, whereas ours learns to generate the values.
\texttt{Values} is given by the following generative process:
\begin{enumerate}
\item Value Choice:
For each pair of entities and types in our datum of records, we predict a value.
We assume that each record type constrains the values to be members of a finite set.
Thus each record type is assigned a categorical distribution over its respective values,
and the values are drawn independently from that respective distribution.
Each $v_i\sim\Cat(f_{t_i}(e_i))$ is drawn from a Categorical distribution,
whose parameters $f_{t_i}(e_i)$ are output by a neural network $f_{t_i}$ that is shared across record types
and takes as input the entity $e_i$.
%The distribution is represented as
%#$$p(\bv\mid\be,\bt) = \prod_{i=1}^N p(v_i\mid e_i,t_i)$$
\item Record Choice:
Conditioned on our choices of values as well as the given entities and records,
we choose a sequence of records $\ba = \set{a_1,\ldots,a_I}$ to describe,
given by their index $a_i$.
Note that each record choice is described by at least one token, hence we have $I \le T$.
The record choices are parameterized as a Markov model where each
$a_t\sim\Cat(f_\theta(a_{t-1},\bv,\be,\bt))$,
where $f_\theta$ is a neural network.
(NEED TO FIX ALIGNMENTS AND T vs t, HSMM vs HMM)
\item Word Choice:
For each record alignment $a_i$,
we choose a sequence of words $\by_i = \set{y_{i1},\ldots,y_{iJ}}$ to describe the record.
The words are modelled by an autoregressive emission model within each segment
that is aligned to the same record:
$y_{ij}\sim\Cat(f_\theta(\by_{i1:ij},v_{a_i},e_{a_i},t_{a_i}))$,
where $f_\theta$ is another neural network and $\by_{i1:ij}$ is all tokens $y$ from indices
$i1$ to $ij$.
\end{enumerate}
The value and record choices correspond to prior distributions over values $p(\bv\mid\be,\bt)$
and alignments $p(\ba\mid\bv,\be,\bt)$ respectively,
while the word choice model gives us the likelihood of some text given our value and alignment choices $p(\by\mid\ba,\bv,\be,\bt)$.
In this case, we have latent $\bz = \set{\bv,\ba}$ and observed $\bx = \set{\be,\bt}$.
We obtain an information extraction by using the 
\textbf{posterior} distribution over alignments and values:
\begin{linenomath*}
$$
p(\bz\mid\by,\bx)=\frac{p(\by,\bz\mid\bx)}{p(\by\mid\bx)}=\frac{p(\by,\bz\mid\bx)}{\sum_\bz p(\by,\bz\mid\bx)}.
$$
\end{linenomath*}
Although the HSMM formulation allows the summation (marginalization) over alignments to be carried out efficiently,
we cannot marginalize over value assignments.
We instead resort to variational inference as in \citet{deng2018attn},
where we learn an approximation of the posterior distribution $q(\bz\mid\by,\bx)$
with a separate model.
% Can marginalize over alignments, so only a partial variational approximation is needed
Given that we are primarily interested in the values rather than the alignments,
we can obtain an information extraction system over only values by marginalizing over alignments.
By marginalizing over the alignment distribution, the model propagates uncertainty over alignments
to uncertainty over values.
We can train this model by maximizing a lower bound on the log marginal likelihood or evidence of $\by$,
called the evidence lower bound (ELBO) which is given formally by the expression
$\textrm{ELBO}_q \triangleq \Es{q(\bz\mid\by,\bz)}{\log p(\by\mid\bz,\bx)}\leq \log \Es{p(\bz\mid\bx)} {p(\by\mid\bz,\bx)}$.
This objective can be maximized with gradient-based methods.
The resulting approximate posterior $q(\bz\mid\by,\bx)$ can be used independently of the 
generative model as an information extraction system that gives a distribution over
values in a table of records and alignments from text to records.

As we are interested in applying the LVM framework to information extraction rather than a single model,
we proceed to motivate and outline an extension to \texttt{Values}.
In addition to inferring values, ideally an information extraction system 
would also be able to infer new entities and relation types.
We propose a possible route towards defining model that can
learn to parameterize new relation types in an
unsupervised manner using the same LVM framework as \texttt{Values},
which we denote \texttt{Types}.
The motivation behind \texttt{Types} is that a segment of text may refer to multiple records at the same time.
For example, in basketball games a `triple-double' refers to more a player achieving a value of more than 10
in any three of the five categories: points, steals, rebounds, blocks, or assists.
The goal of \texttt{Types} is to attempt to capture the latent relationship behind utterances
such as `triple-double' appearing in the text and the underlying records in an unsupervised manner.
We plan to introduce a new step to the generative process of \texttt{Values} that 
allows the model to learn new records as boolean-valued functions of relations already defined in the data.
By approximating these boolean functions with neural networks, we hope to find a model
parameterization that admits a low variance Monte Carlo gradient estimator.

%TODO: Experiments, Evaluation, and Expectations? Also shortcomings? Short paragraph
We will evaluate our initial approach on the Rotowire dataset, and
extensions to our model that will include entity tracking and event resolution on the 
automatic content extraction (ACE) \citep{ace2004} and the Text Analysis Conference's
Streaming Multimedia Knowledge Base Population (SM-KBP) datasets.
We expect the variance of the gradient estimator to be an issue, in particular its
effect on sample complexity. In previous work, we observed that gradient estimators
based on exact inference resulted in better sample complexity than
Monte Carlo gradient estimators \citep{deng2018attn}, and we expect that controlling the variance 
of the gradient estimator with the inductive bias of neural architectures will be of paramount
importance for the success of our proposed method.

Given admittance to the NDSEG Fellowship Program, I will evaluate the application of
LVMs to the problem of information extraction.
As a result of the digital age, the ubiquity of information networks as well as their 
enormous growth makes it clear that a method for training information extraction systems
with minimal supervision is a necessity.
I will push for scalable information extraction systems that require minimal supervision
by recasting information extraction as a generative modeling problem.

\newpage

\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item Importance of representation and correctness of knowledge in decision making,
            as well as the power of prediction.
        \item Information networks are one way of representing knowledge as a graph
            as well as interactions?
        \item What are information networks?
            Graphical representation of objects, their characteristics of interest,
            and their relationship to objects.
        \item Recent work focuses on learning structure between nodes
            in order to fill parts of the graph that may be missing \cite{chen2018diva}.
        \item We focus on the orthogonal approach of completing knowledge graphs using
            large amounts of unstructured text rather than the relationships between nodes.
        \item Reduce information networks to knowledge bases by removing edges
            (kind of interesting as we can view knowledge bases as a mean field approximation
            to information networks where info networks must specify not only
            conditional distributions but also interventional distributions).
        \item In this proposal we present a framework towards automating the
            training of text-centric information extraction systems with minimal supervision.
        \item Shine some hope on interventional distribution representations?
            (This might be a little difficult)
        \end{enumerate}
    \item Supervision in Information Extraction
        \begin{enumerate}
        \item Define information extraction
            \begin{enumerate}
            \item The goal is to produce structured representations of information from
                a given unstructured text.
            \item Ideally, but not necessarily computer-readable in addition to human-readable.
            \item A typical pipeline for information extraction includes
                text segmentation, named entity recognition, coreference resolution,
                relation extraction, and finally producing structured representations of the unlabeled text.
            \item This is knowledge-base completion.
            \item We are primarily interested in knowledge-base completion,
                as the other tasks such named entity recognition are typically
                part of a pipeline aimed at knowledge-base completion.
            \end{enumerate}
        \item Argue for LVM approach to unify all parts of the pipeline and train
            end-to-end with minimal supervision.
        \item We aim to unify these two approaches through recently developed techniques for training LVMs
            parameterized with neural networks.
        \end{enumerate}
    \item End-to-end trends and LVMs
        \begin{enumerate}
        \item Draw parallels to progress in translation, mainly in terms of structure?
            \begin{enumerate}
            \item Statistical machine translation: pipelines
            \item End-to-end with Sutskever
            \item Deterministic structure via attention Bahdanau
            \item Latent attention \citet{deng2018attn}
            \end{enumerate}
        \item Leverage modularity of both the neural network and LVM frameworks
            to incorporate more of the pipeline into a joint training framework.
        \item \citet{qu2017ssre} Semi-supervised relation extraction (which is pretty much similar)
            How is what I want different? Capitalize on the progress of powerful generative models,
            can argue that their model is much weaker in terms of generative power,
            but the framework is similar (they perform coordinate ascent in a LVM).
        \end{enumerate}
    \item Recent advances in neural LVMs
        \begin{enumerate}
        \item Semi-supervised LVMs \citet{kingma2014ssvae}?
        \item Demonstrate that parameterization with a neural network does not affect computational
            complexity of inference.
        \item Then the same technique can be applied to model with more structure,
            as long as the graphical model itself permits tractable inference.
        \item In this proposal, we focus on the hidden semi-Markov model (HSMM),
            used in \citet{liang2009semalign} for the task of aligning segments of text to
            records in a knowledge base without supervision. 
        \item As in \citet{liang2009semalign}, we are interested in learning a generative model of text so that
            we can minimize the amount of supervision necessary for training an
            information extraction system.
        \item Also that although worse sample complexity, using an approximate posterior
            with monte carlo sampling achieves comparable performance.
        \end{enumerate}
    \end{enumerate}
\item Background and Notation
    \begin{enumerate}
    \item Formal notation for elements of the dataset
    \item Define the distribution we would like to learn: $p(z\mid y, x)$.
        $z$ and $x$ are placeholders and will change, but $y$ is always the text.
    \item Link to rotowire example
        (argument is that ACE is made up of ontonotes-like sentences, so all short-form)
    \item Clarify that the scope of posterior inference is very general.
    \end{enumerate}
\item Proposal
    \begin{enumerate}
    \item Outline approach
        \begin{enumerate}
        \item Choose a subset of available data as conditioning,
            and thus it is not modelled.
        \item The joint distribution of the remaining variables,
            both observed and unobserved, will be modelled.
        \end{enumerate}
    \item Link back to motivation. We want to scale information extraction
        by requiring less supervision.
    \item We present one model as an example, then later demonstrate how the framework
        can handle extensions of the model.
    \item Define generative model: HSMM as in \citep{liang2009semalign},
        and \citep{wiseman2018template}.
        The generative story (a picture would be helpful):
        \begin{enumerate}
        \item Fill in values
        \item Choose alignments
        \item Choose words
        \end{enumerate}
    \item Define IE as the distribution we would like to learn
        \begin{enumerate}
        %\item Align: $p(c\mid\by,\br)$
        \item Values: $p(c,\bv\mid\by,\be,\bt)$ (Just this one)
        %\item ??: $p()$
        \end{enumerate}
    \item We either use the posterior distribution of the conditional model
        or learn an approximation of it.
    \item Argue that the segmental model encourages more coverage
        by adding pairwise dependencies between labelings \citep{liang2009semalign}.
        We will check how much structure in the generative model
        aids information extraction.
    \item Training and Inference
        \begin{enumerate}
        \item As we are dealing with documents of significant length,
            we train with an approximate posterior in order to satisfy memory constraints.
        \item Highlight that the approx posterior is a SEPARATE model
            that can be used completely independently from generative model,
            i.e. we throw away generative model after training.
        \item We maximize a lower bound on the log marginal likelihood,
            called the evidence lower bound.
        \end{enumerate}
    \item Extension, \texttt{Types}
        \begin{enumerate}
        \item Introduce new step in generative process
        \item Learn a boolean function that composes predicates applied to existing
            records
        \item The search space is very large, so we must either constrain our model 
            in a very clever way or obtain 
            large amounts of data as any stochastic gradient estimator will have very high variance
        \end{enumerate}
    \item Extensions
        \begin{enumerate}
        \item learn new types as functions of existing ones
        \item learn a randomly initialized embedding of the type
            and a neural network directly to predict the value
        \item let the input distribution be 
        \end{enumerate}
    \item Experiments, evaluation, and expectation
        \begin{enumerate}
        \item Evaluate on Rotowire? Highlight long-form text
        \item Also on ACE
        \item As corpora may be too large, we might need more hierarchy in the generative model
        \item and also since memory is linear in the length of the sequence, we may have
            to resort to approximate inference. We can optimize a lower bound
            on the marginal likelihood with an approximation of the
            posterior distribution \citep{deng2018attn}.
        \item Evaluation metrics: for slot-filling, we evaluate 
            the 
        \end{enumerate}
    \item Conclusion
        \begin{enumerate}
        \item Please accept!
        \item Recap: Minimal supervision IE systems so that they can scale to
            extracting information for large information networks from large bodies of text.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\newpage
\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

