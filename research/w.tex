\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\begin{comment}
Spectrum from hard attention => hard segmental => HSMM for generative model, coverage / recall
Should we go into HSMM??? or is hard segmental attention enough

composition function = categorical over two entries of x

structured attention for IE posterior?
\end{comment}

\paragraph{Keywords}
information networks, natural language processing, information extraction, latent variable models

\paragraph{Relevance to Army BAA: II. A. c. iii. (3) Information Networks}
In order to provide decision makers with the information
necessary to make informed choices as well as predict the effects of those choices,
we must have an efficient representation of relevant information and a predictive model
for the resultant state of the world given a choice.
We define information networks as graphs where each node represents a set of facts
and an edge describes how the facts in one node influence the facts in another.
Information networks provide an intuitive and queryable representation of knowledge.
A decision maker must simply query the relevant nodes to gain context, and
when simulating a decision that alters the information in one node
the graph can easily propagate those changes by virtue of its representation.

Given that an information network must represent an extremely large number of facts and relationships,
it is infeasible to specify these completely by hand.
Many recent works have focused on learning to fill in the missing edges of an information network
by recognizing patterns in fully-labeled subgraphs in order to predict whether there should
be an edge between two nodes in an unlabeled subgraph \citep{chen2018diva}.
By filling in missing edges, we are able to use the relationships specified by the edges
in order to reason about the facts contained in a node conditioned on the facts of its
neighbours.

We propose an orthogonal approach which instead relies on jointly modeling
text as well as the nodes of the information network.
We aim to leverage copious amounts of unstructured data by
learning an information extraction system that is able to 
extract facts from text in order to fill in the nodes of an information network.
In this proposal, we present a method towards automating the
training of information extraction systems with unlabeled corpora
by recasting the information extraction problem as a generative modeling problem.

\paragraph{Introduction}
% Define generative model?
Since the addition of neural networks to the practitioner's toolbox,
extracting information from text has been cast as a supervised classification problem.
In fact, in many tasks where generative models were once used, such as coreference and slot-filling,
recent approaches make progress by fitting large models to large, labelled datasets \citep{lee2018coref,zhang2017slotfilling}.
Although this does result in progress, this particular strategy is not scalable as acquiring supervision is expensive
and time-consuming.
An orthogonal strategy is to search for a class of models that is able to learn well under less supervision.
Latent variable models (LVMs) in particular lend themselves to label-efficient semi-supervised learning.
A LVM is a model which includes both observed and unobserved (latent) random variables.
Past work on coreference and slot-filling utilized LVMs in order to train with distant supervision.
More specifically, \citet{haghighi2010coref} learn a semi-supervised generative model of mentions given mention
segmentations and a relatively small set of labels,
while \citet{surdeanu2012miml} learn to extract the relationships between two entities with noisy labels.
We propose to unify past work on LVMs for information extraction with the powerful modeling capabilities
of neural networks.

Recent work has demonstrated that LVMs parameterized with neural networks can be trained efficiently.
In machine translation, we demonstrated in \citet{deng2018attn} that formulating attention as
a latent variable provides a boost in performance without increasing the computational complexity of training.
The specific model we used did not incorporate interactions between the latent variables,
however the technique we used generalizes to classes of LVMs that do specify interactions.
In this proposal, we focus on the class of LVMs known as hidden semi-Markov models (HSMMs),
used in \citet{liang2009semalign} as a generative model for the task of aligning segments of text to
records in a knowledge base without supervision.
As in \citet{liang2009semalign}, we are interested in learning a generative model of text so that
we can minimize the amount of supervision necessary for training an information extraction system.
The performance of their generative model of text provides signal for learning the alignments:
if the likelihood of a segment of text improves when moving from a past alignment choice to a new one,
then the new alignment is more likely to be correct given a suitably strong likelihood model.
We use that same intuition to formulate a related LVM for weakly supervised information extraction
which aims to model not just the alignments from segments of text to records in a knowledge base
but also the values in the knowledge base itself.
By parameterizing the generative HSMM with neural networks as in \citet{wiseman2018template},
we hope to incorporate recent progress in parameterizing LVMs with neural networks
so as to learn a more accurate information extraction system by using a more powerful
generative model -- taking care to ensure that the model can be trained with minimal supervision.

\paragraph{Background}
We consider datasets consisting of aligned data and text
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single datum and text as $\br,\by$, omitting the superscript.
Each datum $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, where each record $r_i = (e_i, t_i, v_i)$
is a tuple containing an entity, type, and value.
The datum $\br$ is a knowledge base, or equivalently an information network
without a representation of the causal effects between records.
We refer collectively to all the entities, types, and values in a given datum $\br$ as
$\be,\bt,\bv$ respectively.
Each text $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens each from a vocbulary $V$.

Let variables $\bz$ be unobserved or latent, $\by$ observed, and $\bx$ taken as conditioning
and thus not modelled.
For information extraction we are interested in distributions that can be specified as $p(\bz\mid\by,\bx)$,
where $\bz$ and $\bx$ may correspond to various quantities depending on the task
but $\by$ is always the text.

As a concrete example, we use the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games $\by$ aligned with the respective
box scores $\br$ of those games.
Consider a datum that consists of a single record,
$\br = \set{(e_1 = \textrm{Jeremy\_Lin}, t_1 = \textrm{POINTS}, v_1 = 19)}$,
and a simple statement $\by = $``Jeremy Lin scored 19 points''.
In its simplest incarnation, the process of information extraction may be to infer any
subset of $\br$, which in this case will be our latent $\bz$, given the remaining elements in $\br$
which corresponds to $\bx$, as well as the text $\by$.
For example, we may want to infer the value $v_1$ given 
the entity Jeremy Lin, the type POINTS, as well as the text $\by$.
In this case, we would have $\bz = \set{v_1}$ and $\bx = \set{e_1,t_1}$.
In an alternative task, we may want to infer the value $v_1$ 
as well as the type $t_1$ given $\by$ and $e_1$, therefore $\bz = \set{v_1,t_1}$ and $\bx=\set{e_1}$.
%(Will switch to ACE dataset after I figure out what's going on with the data.)

Note that we are not constrained to setting $\bz$ to subsets of $\br$.
We also consider the case where $\bz$ includes alignments from individual words $y_t$
to records $r_i$. We denote the alignments $\ba = \set{a_1,\ldots,a_T}$,
where each $a_t$ is associated with $y_t$ and selects a record $r_i$ such that $a_t = i$.
(NEED TO FIX ALIGNMENTS AND T vs t, HMM vs HSMM)

\paragraph{Proposal}
We propose to verify the efficacy of the LVM framework in the
weakly supervised information extraction setting,
with the goal of demonstrating strong extractive performance with minimal labels.
We present one instance of a LVM and outline how it can be used to obtain
an information extraction model without direct supervision,
then argue that the same approach can be applied in even more ambitious settings.
Our first LVM is a conditional model that specifies
the relationship between data, specifically the entities and types, and text.
We denote this model \texttt{Values}.
%Let $\by$ be the text, $\ba$ be a latent variable that represents the
%alignments from words to records,
%$\bv$ all the values in a datum of records,
%and $\be,\bt$ the entities and types respectively.
Similar to the models defined in \citet{wiseman2018template} and \citet{liang2009semalign},
our model takes the form of a hidden semi-Markov model (HSMM).
The primary difference is that the other models simply assumed the records
were complete and conditioned on them, whereas ours learns to generate the values.
\texttt{Values} is given by the following generative process:
\begin{enumerate}
\item Value Choice:
For each pair of entities and types in our datum of records, we predict a value.
We assume that each record type constrains the values to be members of a finite set.
Thus each record type is assigned a categorical distribution over its respective values,
and the values are drawn independently from that respective distribution.
Each $v_i\sim\Cat(f_{t_i}(e_i))$ is drawn from a Categorical distribution,
whose parameters $f_{t_i}(e_i)$ are output by a neural network $f_{t_i}$ that is shared across record types
and takes as input the entity $e_i$.
%The distribution is represented as
%#$$p(\bv\mid\be,\bt) = \prod_{i=1}^N p(v_i\mid e_i,t_i)$$
\item Record Choice:
Conditioned on our choices of values as well as the given entities and records,
we choose a sequence of records $\ba = \set{a_1,\ldots,a_I}$ to describe,
given by their index $a_i$.
Note that each record choice is described by at least one token, hence we have $I \le T$.
The record choices are parameterized as a Markov model where each
$a_t\sim\Cat(f_\theta(a_{t-1},\bv,\be,\bt))$,
where $f_\theta$ is a neural network.
(NEED TO FIX ALIGNMENTS AND T vs t, HSMM vs HMM)
\item Word Choice:
For each record alignment $a_i$,
we choose a sequence of words $\by_i = \set{y_{i1},\ldots,y_{iJ}}$ to describe the record.
The words are modelled by an autoregressive emission model within each segment
that is aligned to the same record:
$y_{ij}\sim\Cat(f_\theta(\by_{i1:ij},v_{a_i},e_{a_i},t_{a_i}))$,
where $f_\theta$ is another neural network and $\by_{i1:ij}$ is all tokens $y$ from indices
$i1$ to $ij$.
\end{enumerate}
The value and record choices correspond to prior distributions over values $p(\bv\mid\be,\bt)$
and alignments $p(\ba\mid\bv,\be,\bt)$ respectively,
while the word choice model gives us the likelihood of some text given our value and alignment choices $p(\by\mid\ba,\bv,\be,\bt)$.
In this case, we have latent $\bz = \set{\bv,\ba}$ and observed $\bx = \set{\be,\bt}$.
We obtain an information extraction by using the 
\textbf{posterior} distribution over alignments and values:
\begin{linenomath*}
$$
p(\bz\mid\by,\bx)=\frac{p(\by,\bz\mid\bx)}{p(\by\mid\bx)}=\frac{p(\by,\bz\mid\bx)}{\sum_\bz p(\by,\bz\mid\bx)}.
$$
\end{linenomath*}
Although the HSMM formulation allows the summation (marginalization) over alignments to be carried out efficiently,
we cannot marginalize over value assignments.
We instead resort to variational inference as in \citet{deng2018attn},
where we learn an approximation of the posterior distribution $q(\bz\mid\by,\bx)$
with a separate model.
% Can marginalize over alignments, so only a partial variational approximation is needed
Given that we are primarily interested in the values rather than the alignments,
we can obtain an information extraction system over only values by marginalizing over alignments.
By marginalizing over the alignment distribution, the model propagates uncertainty over alignments
to uncertainty over values.
We can train this model by maximizing a lower bound on the log marginal likelihood or evidence of $\by$,
called the evidence lower bound (ELBO) which is given formally by the expression
$\textrm{ELBO}_q \triangleq \Es{q(\bz\mid\by,\bz)}{\log p(\by\mid\bz,\bx)}\leq \log \Es{p(\bz\mid\bx)} {p(\by\mid\bz,\bx)}$.
This objective can be maximized with gradient-based methods.
The resulting approximate posterior $q(\bz\mid\by,\bx)$ can be used independently of the 
generative model as an information extraction system that gives a distribution over
values in a table of records and alignments from text to records.

As we are interested in applying the LVM framework to information extraction rather than a single model,
we proceed to motivate and outline an extension to \texttt{Values}.
In addition to inferring values, ideally an information extraction system 
would also be able to infer new entities and relation types.
We propose a possible route towards defining model that can
learn to parameterize new relation types in an
unsupervised manner using the same LVM framework as \texttt{Values},
which we denote \texttt{Types}.
The motivation behind \texttt{Types} is that a segment of text may refer to multiple records at the same time.
For example, in basketball games a `triple-double' refers to more a player achieving a value of more than 10
in any three of the five categories: points, steals, rebounds, blocks, or assists.
The goal of \texttt{Types} is to attempt to capture the latent relationship behind utterances
such as `triple-double' appearing in the text and the underlying records in an unsupervised manner.
We plan to introduce a new step to the generative process of \texttt{Values} that 
allows the model to learn new records as boolean-valued functions of relations already defined in the data.
By approximating these boolean functions with neural networks, we hope to find a model
parameterization that admits a low variance Monte Carlo gradient estimator.

%TODO: Experiments, Evaluation, and Expectations? Also shortcomings? Short paragraph
We will evaluate our initial approach on the Rotowire dataset, and
extensions to our model that will include entity tracking and event resolution on the 
automatic content extraction (ACE) \citep{ace2004} and the Text Analysis Conference's
Streaming Multimedia Knowledge Base Population (SM-KBP) datasets.
We expect the variance of the gradient estimator to be an issue, in particular its
effect on sample complexity. In previous work, we observed that gradient estimators
based on exact inference resulted in better sample complexity than
Monte Carlo gradient estimators \citep{deng2018attn}, and we expect that controlling the variance 
of the gradient estimator with the inductive bias of neural architectures will be of paramount
importance for the success of our proposed method.

Given admittance to the NDSEG Fellowship Program, I will evaluate the application of
LVMs to the problem of information extraction.
As a result of the digital age, the ubiquity of information networks as well as their 
enormous growth makes it clear that a method for training information extraction systems
with minimal supervision is a necessity.
I will push for scalable information extraction systems that require minimal supervision
by recasting information extraction as a generative modeling problem.


\begin{comment}
We denote the distribution over alignments and values given the
text, entities, and types by $p(\bz\mid\by,\bx)$,
where $\bz = \set{\ba, \bv}$ and $\bx = \set{\be, \bt}$.
This distribution $p(\bz\mid\by,\bx)$ is the IE system,
which models the alignments and values given the text, entities, and types.

JUNK PAST THIS POINT.

\paragraph{Old Prop}
Our goal is to maximize the \underline{coverage} of the information extraction system,
which we define as the number of words contained within segments of text that are correctly aligned to data,
while minimizing the amount of supervision needed.
We propose to learn an information extraction (IE) system as the approximate posterior distribution over 
alignments from segments of text to their generating data.
We also propose to view the given data as incomplete
and learn boolean-valued functions of the data as a step
towards representation learning.

\paragraph{Problem Definition}
\begin{enumerate}
\item Define notation
\item Define generation, mention sam and puduppully work.
\item Define extraction and its subtasks, mention regina and percy's work.
\item TODO: maybe combine align and values? Although I prefer to keep them separate
since values will probably turn into fill in the away team's values given home team's
\end{enumerate}
Rotowire consists of aligned box score data and basketball game summaries
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single data and summary as $\br,\by$, omitting the superscript.
Each data $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
We refer collectively to all the entities, types, and values in a given data $\br$ as
$\be,\bt,\bv$ respectively.
Each summary $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens that makes up the
text of the game description.

For generation, the goal is to learn a conditional model $p(\by\mid\br)$ of the text given the data.
This is simple to evaluate, as we can use the log-likelihood of a given summary under our model
as a measure of performance.
In \citet{wiseman2017d2t} the model takes the form of a conditional language model
that can copy values directly from records in $\br$.
Subsequent work in \citet{puduppully2018contentselection} decomposed the distribution 
$p(\by\mid\br) = \sum_{\bc}p(\by\mid\bc)p(\bc\mid\br)$
by introducing a content plan $\bc$, which is a sequence of records drawn from $\br$.
This was also previously implemented in prior work \citep{liang2009semalign},
which modelled the text generation process through a hierarchical hidden semi-Markov model.
% Definitely need a figure here

We divide the information extraction task into three subtasks.
Before outlining the tasks, we propose two measures of performance through which
to evaluate an unsupervised information extraction system.
The first is how well the information extracted from a summary allows a 
generative model to reconstruct the summary measured by the likelihood
of the summary given the extracted information.
We refer to this as \underline{reconstruction}.
The second is \underline{coverage}, 
which we define as the number of words contained in segments
that are aligned to a record in $\br$.
(Do I need to argue why these are useful?
And also how the tasks aim at increasing them by weakening assumptions
or constraining model flexibility compared to previous work)

The first task (\textsc{align}) is to align segments of text to the records that generated them.
This is similar to learning a content plan $\bc\mid\br$ as in \citet{puduppully2018contentselection},
however we are interested in the \textbf{posterior} distribution $p(\bc\mid\br,\by)$ of
the content plan $\bc$ after observing the text $\by$.
\citet{liang2009semalign} utilize the fact that they define a model in which posterior inference is tractable,
however tractability does not hold once the latent distribution becomes autoregressive.
% Elaborate
In \citet{wiseman2017d2t} and subsequently in \citet{puduppully2018contentselection} this was accomplished by 
separately training a classifier to predict the type $t$ of an entity $e$ and value $v$ pair within a sentence.
The entity and value are extracted heuristically by checking exact string matches within $\be$ and $\bv$,
and the supervision over $t$ is obtained through the following function \citep{wiseman2017d2t}:
$\text{findType}(\hat{e},\hat{v}) = \set{ r.t :x\in\br, r.e = \hat{e}, x.r = \hat{v}}$.
However, this limits alignments exclusively to entities and values explicitly in $\br$.
We would like to align whole segments of text in order to increase the coverage
of our information extraction system.

The second task (\textsc{values}) is to reconstruct values $v$ in the table $\br$.
This is implemented on top of task (\textsc{align}).
In particular, we want to learn $p(\bv \mid \by,\bc,\be,\bt)$,
the distribution over all values given the summary, the content plan, all entities, and all types.

The third task (\textsc{functions}) is the most ambitious.
In order to demonstrate the flexibility of the framework,
we propose a method to further learn functions of $\br$ in an unsupervised manner.
(TODO)

\paragraph{Model}
We begin by defining a model for (\textsc{align})
and proceed to (\textsc{values}) and (\textsc{functions}).

% Generative model
Our generative model factors into the
likelihood and prior: $p(\by,\bc\mid\br)=p(\by\mid\bc)p(\bc\mid\br)$.
Our likelihood $p(\by\mid\bc)$ is given by a conditional neural language model
with a copy mechanism as in \citet{gulcehre2016cc,wiseman2017d2t}
in addition to monotonic attention \citep{yu2016ssnt,wiseman2018template}.
The prior $p(\bc\mid\br)$ is an autoregressive model over records
parameterized with an LSTM.
As we are primarily interested in posterior inference, the performance of the prior is
not the most important aspect of the model.
In fact, we will see in the next section that the prior serves to regularize
our approximate posterior.
(This is the simplest baseline aside from HSMM, can include 
$p(\by,\bc\mid\br)=\prod_tp(y_t\mid\by_{<t},c_t)p(c_t\mid\bc_{<t},\by_{<t},\br)$
if necessary)

% IE model
Our initial IE model for (\textsc{align}) is given by $q(\bc\mid\by,\br)$,
which includes both a segmentation of the summary as well as the alignments.
Note that we denote the distribution using $q$ since under the generative model
$p(\bc\mid\by,\br)$ is well-defined as the posterior distribution of alignments
given a summary and records.
Initially we parameterize the approximate posterior
$q(\bc\mid\by,\br)=\prod_tq(c_i\mid\by,\br)$
as a fully factored distribution over alignments.
(Structured attention for pairwise potentials later,
since only has node potentials for now.
This is motivated by \underline{coverage})
Each $q(c_i\mid\by,\br)$ is parameterized by the output
of a BLSTM run at the sentence level.
(TODO: values, functions)

\paragraph{Learning and Inference}
\begin{enumerate}
\item Question: Do I need to motivate approximate inference?
I could also argue that rather than computing the posterior exactly,
at test time using the approximation directly can be more efficient,
especially if it is fully factored
\item Need to figure out if this is the right place for this
\item Describe how to incorporate the weak supervision through posterior constraints
\end{enumerate}
A latent variable model $p(\by,\bz\mid\bx)$ models an
observed $\by$ as well as an unobserved $\bz$ conditioned on $\bx$.
When fitting such a model, we would like to maximize the evidence
$p(\by\mid\bx)=\sum_\bz p(\by,\bz\mid\bx)$ which marginalizes over
the latent $\bz$.
Depending on the structure of $p(\by,\bz\mid\bx)$,
the marginalization procedure may be intractable to perform exactly.
For example, this is the case with an autoregressive model for the latent
variable $p(z_t\mid \bz_{<t})$, where variable elimination's runtime would be
exponential in the length $|\bz|$.
This also precludes tractable posterior inference, i.e. $p(\bz\mid\by,\bx)$,
since by Bayes' Rule we have $p(\bz\mid\by,\bx)=p(\by,\bz\mid\bx)/p(\by\mid\bx)$
which requires evaluating the intractable sum in the evidence $p(\by\mid\bx)$.
Therefore we resort to learning an approximation of the posterior through the variational principle:
the best approximation within a family of distributions is the one with minimal KL-divergence
to the model's posterior.
The KL between the approximate posterior and true posterior is still intractable to minimize exactly,
so we instead maximize the evidence lower bound ($\mathcal{L}_q$), which is the evidence minus the posterior KL:
\begin{linenomath*}
\begin{align}
\mathcal{L}_q
&= \underbrace{\log p(\by\mid\bx)}_{\textrm{Evidence}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx)||p(\bz\mid\by,\bx)]}_{\textrm{Posterior KL}}\\
&= \underbrace{\Es{\bz\sim q(\bz\mid\by,\bx)}{\log p(\by\mid\bz,\bx)}}_{\textrm{Reconstruction}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx) || p(\bz\mid\bx)]}_{\textrm{Prior KL}}.
\end{align}
\end{linenomath*}
Were it not for local extrema,
maximizing this quantity would maximize the evidence and minimize the posterior KL jointly.
Notice that in (2), the objective we use for training both the generative model and IE system,
all expectations are taken with respect to the IE system or approximate posterior $q(\by\mid\bz,\bx)$.
(TODO: training procedure via REINFORCE + control variate, posterior constraints for incorporating 
information from findType function)

For task (\textsc{align}), we have the fully observed summary $\by$,
the unobserved content plan $\bz=\bc$, and all records as conditioning $\bx=\br$.
For task (\textsc{values}), we again have the observed summary $\by$,
but we pretend the values are unobserved $\bz=\set{\bc,\bv}$, and
use the rest of the records as conditioning $\bx=\set{\be,\bt}$.
(TODO: functions)
\end{comment}

\newpage

\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item Importance of representation and correctness of knowledge in decision making,
            as well as the power of prediction.
        \item Information networks are one way of representing knowledge as a graph
            as well as interactions?
        \item What are information networks?
            Graphical representation of objects, their characteristics of interest,
            and their relationship to objects.
        \item Recent work focuses on learning structure between nodes
            in order to fill parts of the graph that may be missing \cite{chen2018diva}.
        \item We focus on the orthogonal approach of completing knowledge graphs using
            large amounts of unstructured text rather than the relationships between nodes.
        \item Reduce information networks to knowledge bases by removing edges
            (kind of interesting as we can view knowledge bases as a mean field approximation
            to information networks where info networks must specify not only
            conditional distributions but also interventional distributions).
        \item In this proposal we present a framework towards automating the
            training of text-centric information extraction systems with minimal supervision.
        \item Shine some hope on interventional distribution representations?
            (This might be a little difficult)
        \end{enumerate}
    \item Supervision in Information Extraction
        \begin{enumerate}
        \item Recent neural approaches treat information extraction as a classification problem
            with little intermediate structure.
        \item 
        \item Older approaches utilized latent variable models to capitalize on intermediate structure,
            however this was before neural models were used in NLP.
            \citep{surdeanu2012miml,liang2009semalign}
        \item We aim to unify these two approaches through recently developed techniques for training LVMs
            parameterized with neural networks.
        \end{enumerate}
    \item Recent advances in neural LVMs
        \begin{enumerate}
        \item Demonstrate that parameterization with a neural network does not affect computational
            complexity of inference.
        \item Then the same technique can be applied to model with more structure,
            as long as the graphical model itself permits tractable inference.
        \item In this proposal, we focus on the hidden semi-Markov model (HSMM),
            used in \citet{liang2009semalign} for the task of aligning segments of text to
            records in a knowledge base without supervision. 
        \item As in \citet{liang2009semalign}, we are interested in learning a generative model of text so that
            we can minimize the amount of supervision necessary for training an
            information extraction system.
        \item Also that although worse sample complexity, using an approximate posterior
            with monte carlo sampling achieves comparable performance.
        \end{enumerate}
    \end{enumerate}
\item Background and Notation
    \begin{enumerate}
    \item Formal notation for elements of the dataset
    \item Define the distribution we would like to learn: $p(z\mid y, x)$.
        $z$ and $x$ are placeholders and will change, but $y$ is always the text.
    \item Link to rotowire example
        (argument is that ACE is made up of ontonotes-like sentences, so all short-form)
    \item Clarify that the scope of posterior inference is very general.
    \end{enumerate}
\item Proposal
    \begin{enumerate}
    \item Outline approach
        \begin{enumerate}
        \item Choose a subset of available data as conditioning,
            and thus it is not modelled.
        \item The joint distribution of the remaining variables,
            both observed and unobserved, will be modelled.
        \end{enumerate}
    \item Link back to motivation. We want to scale information extraction
        by requiring less supervision.
    \item We present one model as an example, then later demonstrate how the framework
        can handle extensions of the model.
    \item Define generative model: HSMM as in \citep{liang2009semalign},
        and \citep{wiseman2018template}.
        The generative story (a picture would be helpful):
        \begin{enumerate}
        \item Fill in values
        \item Choose alignments
        \item Choose words
        \end{enumerate}
    \item Define IE as the distribution we would like to learn
        \begin{enumerate}
        %\item Align: $p(c\mid\by,\br)$
        \item Values: $p(c,\bv\mid\by,\be,\bt)$ (Just this one)
        %\item ??: $p()$
        \end{enumerate}
    \item We either use the posterior distribution of the conditional model
        or learn an approximation of it.
    \item Argue that the segmental model encourages more coverage
        by adding pairwise dependencies between labelings \citep{liang2009semalign}.
        We will check how much structure in the generative model
        aids information extraction.
    \item Training and Inference
        \begin{enumerate}
        \item As we are dealing with documents of significant length,
            we train with an approximate posterior in order to satisfy memory constraints.
        \item Highlight that the approx posterior is a SEPARATE model
            that can be used completely independently from generative model,
            i.e. we throw away generative model after training.
        \item We maximize a lower bound on the log marginal likelihood,
            called the evidence lower bound.
        \end{enumerate}
    \item Extension, \texttt{Types}
        \begin{enumerate}
        \item Introduce new step in generative process
        \item Learn a boolean function that composes predicates applied to existing
            records
        \item The search space is very large, so we must either constrain our model 
            in a very clever way or obtain 
            large amounts of data as any stochastic gradient estimator will have very high variance
        \end{enumerate}
    \item Extensions
        \begin{enumerate}
        \item learn new types as functions of existing ones
        \item learn a randomly initialized embedding of the type
            and a neural network directly to predict the value
        \item let the input distribution be 
        \end{enumerate}
    \item Experiments, evaluation, and expectation
        \begin{enumerate}
        \item Evaluate on Rotowire? Highlight long-form text
        \item Also on ACE
        \item As corpora may be too large, we might need more hierarchy in the generative model
        \item and also since memory is linear in the length of the sequence, we may have
            to resort to approximate inference. We can optimize a lower bound
            on the marginal likelihood with an approximation of the
            posterior distribution \citep{deng2018attn}.
        \item Evaluation metrics: for slot-filling, we evaluate 
            the 
        \end{enumerate}
    \item Conclusion
        \begin{enumerate}
        \item Please accept!
        \item Recap: Minimal supervision IE systems so that they can scale to
            extracting information for large information networks from large bodies of text.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

