\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\begin{comment}
Spectrum from hard attention => hard segmental => HSMM for generative model, coverage / recall
Should we go into HSMM??? or is hard segmental attention enough

composition function = categorical over two entries of x

structured attention for IE posterior?
\end{comment}

\paragraph{Keywords}
information networks, natural language processing, information extraction,
latent variable models

\paragraph{Relevance to BAA}
As information networks grow larger and more complex,
acquiring explicit supervision for the training of information extraction systems
becomes extremely expensive.
Information networks are also dynamic;
over time one method for representing information may become inadequate.
Currently, both the representation of the information in a knowledge base
as well as the extraction process itself must be hand-designed.
In this proposal we present a framework towards automating the
training of information extraction systems with minimal supervision.

\paragraph{Introduction}
% Define generative model?
Natural language processing contains two separate but
closely related subfields: natural language understanding (NLU) and natural language
generation (NLG).
Recent approaches to information extraction use only the NLU perspective
and frame extraction as a classification problem.
We argue that the two perspectives, NLU and NLG, are complementary
and capitalize on their duality by proposing a method to train a NLU system
without direct supervision.
%More specifically, we utilize the latent variable model framework to train a 
%neural network-based information extraction system and use as signal 
More specifically, we train an information extraction system by using
the performance of a deep generative model as signal.
By training in this fashion we obtain an information extraction system
that extracts facts that best explain the given text.

%\paragraph{Supervision in NLU}
Recent work using neural network-based systems cast information extraction
as a supervised problem
\citep{wiseman2017d2t,dossantos2015classification}.
Although approaches do incorporate structure (?) into internal representations,
for example to integrate multiple sources of information \citep{weissenborn17},
the final output still uses strong supervision during training.
Finding strong supervision is a difficult task, and as datasets get larger
we must approach information extraction from a different perspective.
Latent variable models (LVMs) are one method for alleviating the need for supervision.
LVMs do not require manual labels, as they instead treat quantities of interest
as latent random variables and deal with them in a probabilistically principled 
fashion.
We will leverage recent advances in deep generative modeling in order to
train without explicit supervision.

%\paragraph{Recent advances in NLG}
Neural models for language generation have seen much progress in recent years,
with state of the art performance in both language modeling and translation
\citep{yang2017moslm,allattn}.
By integrating the powerful language modeling capabilities of neural networks with
the inductive biases of graphical models through LVMs,
we can leverage the LVM framework to derive a principled method
for training an information extraction system with less supervision.
Namely, we turn to an efficient technique for training hard attention 
that relies on variational inference \citep{deng2018attn}.
This technique is applicable to sequential LVMs such as hidden Markov models 
and hidden semi-Markov models (HSMMs).
HSMMs have been applied to small scale datasets for text generation \citep{wiseman2018template}
as well as without integrating neural networks \citep{liang2009semalign}.
We will scale sequential LVMs to large datasets through variational inference and use
the signal from our generative model in order to train an information extraction system.

%Modern neural systems have demonstrated state-of-the-art performance on short-form
%text generation tasks, but the performance seemingly fails to generalize to long-form generation.
%D2T is a task that isolates performance on long-form generation by disentangling 
%generation quality from input representation.
\paragraph{Background}
We consider datasets consisting of aligned data and text
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single datum and text as $\br,\by$, omitting the superscript.
Each datum $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
The datum $\br$ is a flattened representation of an information network.
Whereas an information network may also be represented as a hypergraph,
$\br$ is a list of relations or records.
We refer collectively to all the entities, types, and values in a given datum $\br$ as
$\be,\bt,\bv$ respectively.
Each text $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens.

Let variables $\bz$ be unobserved or latent, $\by$ observed, and $\bx$ taken as conditioning
and thus not modelled.
For information extraction we are interested in distributions that can be specified as $p(\bz\mid\by,\bx)$,
where $\bz$ and $\bx$ may correspond to various quantities depending on the task
but $\by$ is always the text.

As a concrete example, we use the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games $\by$ aligned with the respective
box scores $\br$ of those games.
Consider a datum that consists of a single record,
$\br = \set{(e_1 = \textrm{Jeremy Lin}, t_1 = \textrm{POINTS}, v_1 = 19)}$,
and a simple statement $\by = $``Jeremy Lin scored 19 points''.
In its simplest incarnation, the process of information extraction may be to infer any
subset of $\br$, which in this case will be our latent $\bz$, given the remaining elements in $\br$
which corresponds to $\bx$, as well as the text $\by$.
For example, we may want to infer the type of the relation $t$ given 
the entity Jeremy Lin, the value 19, as well as the text $\by$.
In this case, we would have $\bz = \set{t_1}$ and $\bx = \set{e_1,v_1}$.
In an alternative task, we may want to infer the value $v_1$ 
as well as the type $t_1$ given $\by$ and $e_1$, therefore $\bz = \set{t_1,v_1}$ and $\bx=\set{e_1}$.
(Will switch to ACE dataset after I figure out what's going on with the data.)

Note that we are not constrained to setting $\bz$ to subsets of $\br$.
We also consider the case where $\bz$ includes alignments from individual words $y_t$
to records $r_i$. We denote the alignments $\ba = \set{a_1,\ldots,a_T}$,
where each $a_t$ is associated with $y_t$ and selects a record $r_i$ such that $a_t = i$.

\paragraph{Proposal}
We propose to verify the efficacy of the LVM framework in the
weakly supervised information extraction setting,
with the goal of demonstrating strong extractive performance with minimal labels.
We present one instance of a LVM and outline how it can be used to obtain
an information extraction model without direct supervision,
then argue that the same approach can be applied in even more ambitious settings.
Our first LVM is a conditional model that specifies
the relationship between data, specifically the entities and types, and text.
We denote this model \texttt{Values}.
%Let $\by$ be the text, $\ba$ be a latent variable that represents the
%alignments from words to records,
%$\bv$ all the values in a datum of records,
%and $\be,\bt$ the entities and types respectively.
Similar to the models defined in \citet{wiseman2018template} and \citet{liang2009semalign},
our model takes the form of a hidden semi-Markov model (HSMM).
The primary difference is that the other models simply assumed the records
were complete and conditioned on them, whereas ours learns to generate the values.
\texttt{Values} is given by the following generative process:
\begin{enumerate}
\item Value Choice:
For each pair of entities and types in our datum of records, we predict a value.
We assume that each record type constrains the values to be members of a finite set.
Thus each record type is assigned a categorical distribution over its respective values,
and the values are drawn independently from that respective distribution.
Each $v_i\sim\Cat(f_{t_i}(e_i))$ is drawn from a Categorical distribution,
whose parameters $f_{t_i}$ are output by a neural network that is shared across record types
and takes as input the entity.
%The distribution is represented as
%#$$p(\bv\mid\be,\bt) = \prod_{i=1}^N p(v_i\mid e_i,t_i)$$
\item Record Choice:
Conditioned on our choices of values as well as the given entities and records,
we choose a sequence of records $\ba = \set{a_1,\ldots,a_I}$ to describe.
Each record alignment $a_i$ points to the index of its corresponding record.
The records are parameterized as a Markov model where each
$a_t\sim\Cat(f_\theta(a_{t-1},\bv,\be,\bt))$,
where $f_\theta$ is a neural network.
%We parameterize this with a first-order Markov model:
%$$p(\ba\mid\bv,\be,\bt) = \prod_{t=1}^T p(a_t\mid a_{t-1},\bv,\be,\bt)$$
\item Word Choice:
For each record alignment $a_i$,
we choose a sequence of words $\by_i = \set{y_{i1},\ldots,y_{iJ}}$ to describe the record.
The words are modelled by an autoregressive emission model within each segment
that is aligned to the same record:
$y_{ij}\sim\Cat(f_\theta(\by_{i<j},v_{a_i},e_{a_i},t_{a_i}))$,
where $f_\theta$ is another neural network.
\end{enumerate}
The value and record choices correspond to prior distributions over values $p(\bv\mid\be,\bt)$
and alignments $p(\ba\mid\bv,\be,\bt)$ respectively,
while the word choice model gives us the likelihood of some text given our value and alignment choices $p(\by\mid\ba,\bv,\be,\bt)$.
In this case, we have latent $\bz = \set{\bv,\ba}$ and observed $\bx = \set{\be,\bt}$.
We obtain an information extraction by using the 
\textbf{posterior} distribution over alignments and values:
\begin{linenomath*}
$$
p(\bz\mid\by,\bx)=\frac{p(\by,\bz\mid\bx)}{p(\by\mid\bx)}=\frac{p(\by,\bz\mid\bx)}{\sum_\bz p(\by,\bz\mid\bx)}.
$$
\end{linenomath*}
Althought the HSMM formulation allows the summation (marginalization) over alignments to be carried out efficiently,
we cannot marginalize over value assignments.
We instead resort to variational inference as in \citet{deng2018attn},
where we learn an approximation of the posterior distribution $q(\bz\mid\by,\bx)$
with a separate model.
% Can marginalize over alignments, so only a partial variational approximation is needed
Given that we are primarily interested in the values rather than the alignments,
we can obtain an information extraction system over only values by marginalizing over alignments.
By marginalizing over the alignment distribution, the model propagates uncertainty over alignments
to uncertainty over values.
We can train this model by maximizing a lower bound on the log marginal likelihood or evidence of $\by$,
called the evidence lower bound (ELBO) which is given formally by the expression
$\textrm{ELBO}_q \triangleq \Es{q(\bz\mid\by,\bz)}{\log p(\by\mid\bz,\bx)}\leq \log \Es{p(\bz\mid\bx)} {p(\by\mid\bz,\bx)}$.
This objective can be maximized with gradient-based methods.
The resulting approximate posterior $q(\bz\mid\by,\bx)$ can be used independently of the 
generative model as an information extraction system that gives a distribution over
values in a table of records and alignments from text to records.

As we are interested in applying the LVM framework to information extraction rather than a single model,
we proceed to motivate and outline an extension to \texttt{Values}.
In addition to inferring values, ideally an information extraction system 
would also be able to infer new entities and relation types.
We propose a possible route towards defining model that can
learn to parameterize new relation types in an
unsupervised manner using the same LVM framework as \texttt{Values},
which we denote \texttt{Types}.
The motivation behind \texttt{Types} is that a text may refer to multiple records at the same time.
For example, in basketball games a `triple-double' refers to more a player achieving a value of more than 10
of any three of the five categories: points, steals, rebounds, blocks, or assists.
The goal of \texttt{Types} is to attempt to capture the latent relationship behind a `triple-double' appearing in 
the text and the underlying records in an unsupervised manner.
We plan to introduce a new step to the generative process of \texttt{Values} that 
allows the model to learn new records as boolean-valued functions of relations already defined in the data.
By approximating these boolean functions with neural networks, we hope to find a model
parameterization that admits a low variance monte carlo gradient estimator.

%TODO: Experiments, Evaluation, and Expectations? Also shortcomings? Short paragraph
We will evaluate our initial approach on the Rotowire dataset, and
extensions to our model that will include entity tracking and event resolution on the 
automatic content extraction (ACE) \citep{ace2004} and the Text Analysis Conference's
Streaming Multimedia Knowledge Base Population (SM-KBP) datasets.
We expect the variance of the gradient estimator to be an issue, in particular its
effect on sample complexity. In previous work, we observed that gradient estimators
based on exact inference resulted in better sample complexity than
monte carlo gradient estimators \citep{deng2018attn}, and we expect that controlling the variance 
of the gradient estimator with the inductive bias of neural architectures will be of paramount
importance for the success of our proposed method.

Given admittance to the NDSEG Fellowship Program, I will evaluate the application of
unsupervised LVMs to the problem of information extraction.
As a result of the digital age, the ubiquity of information networks as well as their 
enormous growth makes it clear that a method for training information extraction systems
with minimal supervision is a necessity.
I will push for scalable information enformation extraction systems that require minimal supervision
by recasting information extraction  as a generative modeling problem.


\begin{comment}
We denote the distribution over alignments and values given the
text, entities, and types by $p(\bz\mid\by,\bx)$,
where $\bz = \set{\ba, \bv}$ and $\bx = \set{\be, \bt}$.
This distribution $p(\bz\mid\by,\bx)$ is the IE system,
which models the alignments and values given the text, entities, and types.

JUNK PAST THIS POINT.

\paragraph{Old Prop}
Our goal is to maximize the \underline{coverage} of the information extraction system,
which we define as the number of words contained within segments of text that are correctly aligned to data,
while minimizing the amount of supervision needed.
We propose to learn an information extraction (IE) system as the approximate posterior distribution over 
alignments from segments of text to their generating data.
We also propose to view the given data as incomplete
and learn boolean-valued functions of the data as a step
towards representation learning.

\paragraph{Problem Definition}
\begin{enumerate}
\item Define notation
\item Define generation, mention sam and puduppully work.
\item Define extraction and its subtasks, mention regina and percy's work.
\item TODO: maybe combine align and values? Although I prefer to keep them separate
since values will probably turn into fill in the away team's values given home team's
\end{enumerate}
Rotowire consists of aligned box score data and basketball game summaries
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single data and summary as $\br,\by$, omitting the superscript.
Each data $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
We refer collectively to all the entities, types, and values in a given data $\br$ as
$\be,\bt,\bv$ respectively.
Each summary $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens that makes up the
text of the game description.

For generation, the goal is to learn a conditional model $p(\by\mid\br)$ of the text given the data.
This is simple to evaluate, as we can use the log-likelihood of a given summary under our model
as a measure of performance.
In \citet{wiseman2017d2t} the model takes the form of a conditional language model
that can copy values directly from records in $\br$.
Subsequent work in \citet{puduppully2018contentselection} decomposed the distribution 
$p(\by\mid\br) = \sum_{\bc}p(\by\mid\bc)p(\bc\mid\br)$
by introducing a content plan $\bc$, which is a sequence of records drawn from $\br$.
This was also previously implemented in prior work \citep{liang2009semalign},
which modelled the text generation process through a hierarchical hidden semi-Markov model.
% Definitely need a figure here

We divide the information extraction task into three subtasks.
Before outlining the tasks, we propose two measures of performance through which
to evaluate an unsupervised information extraction system.
The first is how well the information extracted from a summary allows a 
generative model to reconstruct the summary measured by the likelihood
of the summary given the extracted information.
We refer to this as \underline{reconstruction}.
The second is \underline{coverage}, 
which we define as the number of words contained in segments
that are aligned to a record in $\br$.
(Do I need to argue why these are useful?
And also how the tasks aim at increasing them by weakening assumptions
or constraining model flexibility compared to previous work)

The first task (\textsc{align}) is to align segments of text to the records that generated them.
This is similar to learning a content plan $\bc\mid\br$ as in \citet{puduppully2018contentselection},
however we are interested in the \textbf{posterior} distribution $p(\bc\mid\br,\by)$ of
the content plan $\bc$ after observing the text $\by$.
\citet{liang2009semalign} utilize the fact that they define a model in which posterior inference is tractable,
however tractability does not hold once the latent distribution becomes autoregressive.
% Elaborate
In \citet{wiseman2017d2t} and subsequently in \citet{puduppully2018contentselection} this was accomplished by 
separately training a classifier to predict the type $t$ of an entity $e$ and value $v$ pair within a sentence.
The entity and value are extracted heuristically by checking exact string matches within $\be$ and $\bv$,
and the supervision over $t$ is obtained through the following function \citep{wiseman2017d2t}:
$\text{findType}(\hat{e},\hat{v}) = \set{ r.t :x\in\br, r.e = \hat{e}, x.r = \hat{v}}$.
However, this limits alignments exclusively to entities and values explicitly in $\br$.
We would like to align whole segments of text in order to increase the coverage
of our information extraction system.

The second task (\textsc{values}) is to reconstruct values $v$ in the table $\br$.
This is implemented on top of task (\textsc{align}).
In particular, we want to learn $p(\bv \mid \by,\bc,\be,\bt)$,
the distribution over all values given the summary, the content plan, all entities, and all types.

The third task (\textsc{functions}) is the most ambitious.
In order to demonstrate the flexibility of the framework,
we propose a method to further learn functions of $\br$ in an unsupervised manner.
(TODO)

\paragraph{Model}
We begin by defining a model for (\textsc{align})
and proceed to (\textsc{values}) and (\textsc{functions}).

% Generative model
Our generative model factors into the
likelihood and prior: $p(\by,\bc\mid\br)=p(\by\mid\bc)p(\bc\mid\br)$.
Our likelihood $p(\by\mid\bc)$ is given by a conditional neural language model
with a copy mechanism as in \citet{gulcehre2016cc,wiseman2017d2t}
in addition to monotonic attention \citep{yu2016ssnt,wiseman2018template}.
The prior $p(\bc\mid\br)$ is an autoregressive model over records
parameterized with an LSTM.
As we are primarily interested in posterior inference, the performance of the prior is
not the most important aspect of the model.
In fact, we will see in the next section that the prior serves to regularize
our approximate posterior.
(This is the simplest baseline aside from HSMM, can include 
$p(\by,\bc\mid\br)=\prod_tp(y_t\mid\by_{<t},c_t)p(c_t\mid\bc_{<t},\by_{<t},\br)$
if necessary)

% IE model
Our initial IE model for (\textsc{align}) is given by $q(\bc\mid\by,\br)$,
which includes both a segmentation of the summary as well as the alignments.
Note that we denote the distribution using $q$ since under the generative model
$p(\bc\mid\by,\br)$ is well-defined as the posterior distribution of alignments
given a summary and records.
Initially we parameterize the approximate posterior
$q(\bc\mid\by,\br)=\prod_tq(c_i\mid\by,\br)$
as a fully factored distribution over alignments.
(Structured attention for pairwise potentials later,
since only has node potentials for now.
This is motivated by \underline{coverage})
Each $q(c_i\mid\by,\br)$ is parameterized by the output
of a BLSTM run at the sentence level.
(TODO: values, functions)

\paragraph{Learning and Inference}
\begin{enumerate}
\item Question: Do I need to motivate approximate inference?
I could also argue that rather than computing the posterior exactly,
at test time using the approximation directly can be more efficient,
especially if it is fully factored
\item Need to figure out if this is the right place for this
\item Describe how to incorporate the weak supervision through posterior constraints
\end{enumerate}
A latent variable model $p(\by,\bz\mid\bx)$ models an
observed $\by$ as well as an unobserved $\bz$ conditioned on $\bx$.
When fitting such a model, we would like to maximize the evidence
$p(\by\mid\bx)=\sum_\bz p(\by,\bz\mid\bx)$ which marginalizes over
the latent $\bz$.
Depending on the structure of $p(\by,\bz\mid\bx)$,
the marginalization procedure may be intractable to perform exactly.
For example, this is the case with an autoregressive model for the latent
variable $p(z_t\mid \bz_{<t})$, where variable elimination's runtime would be
exponential in the length $|\bz|$.
This also precludes tractable posterior inference, i.e. $p(\bz\mid\by,\bx)$,
since by Bayes' Rule we have $p(\bz\mid\by,\bx)=p(\by,\bz\mid\bx)/p(\by\mid\bx)$
which requires evaluating the intractable sum in the evidence $p(\by\mid\bx)$.
Therefore we resort to learning an approximation of the posterior through the variational principle:
the best approximation within a family of distributions is the one with minimal KL-divergence
to the model's posterior.
The KL between the approximate posterior and true posterior is still intractable to minimize exactly,
so we instead maximize the evidence lower bound ($\mathcal{L}_q$), which is the evidence minus the posterior KL:
\begin{linenomath*}
\begin{align}
\mathcal{L}_q
&= \underbrace{\log p(\by\mid\bx)}_{\textrm{Evidence}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx)||p(\bz\mid\by,\bx)]}_{\textrm{Posterior KL}}\\
&= \underbrace{\Es{\bz\sim q(\bz\mid\by,\bx)}{\log p(\by\mid\bz,\bx)}}_{\textrm{Reconstruction}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx) || p(\bz\mid\bx)]}_{\textrm{Prior KL}}.
\end{align}
\end{linenomath*}
Were it not for local extrema,
maximizing this quantity would maximize the evidence and minimize the posterior KL jointly.
Notice that in (2), the objective we use for training both the generative model and IE system,
all expectations are taken with respect to the IE system or approximate posterior $q(\by\mid\bz,\bx)$.
(TODO: training procedure via REINFORCE + control variate, posterior constraints for incorporating 
information from findType function)

For task (\textsc{align}), we have the fully observed summary $\by$,
the unobserved content plan $\bz=\bc$, and all records as conditioning $\bx=\br$.
For task (\textsc{values}), we again have the observed summary $\by$,
but we pretend the values are unobserved $\bz=\set{\bc,\bv}$, and
use the rest of the records as conditioning $\bx=\set{\be,\bt}$.
(TODO: functions)
\end{comment}

\newpage

\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item What are information networks?
            Graphical representation of objects, their characteristics of interest, and their relationship to objects.
            Characterized by large graphs and voluminous data.
        \item Information networks must also represent knowledge wihich is both
            uncertain as well as dynamic;
            over time one method for representing information may become inadequate.
        \item Obtaining supervision for large networks is expensive,
            especially if the underlying schema is subject to change.
        \item TODO: Make expensive argument more concrete by hammering in the dynamic schema aspect.
            This argument might be weak since the model would also have to be retrained,
            but that's much cheaper than additional labeling.
        \item TODO: possibly establish a more military-focused
            or at least information-gathering running example, as opposed to basketball.
        \item In this proposal we present a framework towards automating the
            training of information extraction systems with minimal supervision.
        \end{enumerate}
    \item Decomposition of NLP into NLU and NLG
        (Keep this paragraph high-level, just an introduction to the thesis and proposal.)
        \begin{enumerate}
        \item NLP consists of two separate but closely related subfields:
            natural language understanding (NLU) and natural language generation (NLG).
        \item Although both subfields have been dominated by neural network-base models,
            there is a striking difference.
        \item NLU models capitalize primarily on the representational power of neural networks.
            Rather than requiring features as input, neural networks learn representations of
            the data automatically.
            (Maybe move the whole NLU paragraph here, but probably not since I feel like I have a strong opening paragraph.)
        \item However, NLG utilizes neural networks not only for representation learning
            but also for generative modeling.
            Success on tasks such as language modeling and machine translation
            demonstrate that generative and conditional models parameterized by
            neural networks are able to approximate not only simple distributions over labels,
            such as entity or not,
            but also complex joint distributions. (Maybe move the whole generative paragraph here, X)
        \item Recent approaches to information extraction use only the NLU perspective
            and frame extraction as a classification problem, and thus do not take advantage of
            the generative capability of neural networks.
        \item TODO: Possibly give examples QA, information extraction (dos santos + sam).
        \item We argue that the two subfields' approaches are complementary,
            and capitalize on their duality by proposing a method to train a NLU system
            without direct supervision.
        \end{enumerate}
    \item Supervision in NLU
        (Need to hammer in the framing of supervision)
        \begin{enumerate}
        \item Recent work using neural network-based systems cast information extraction
        as a supervised problem
        \citep{wiseman2017d2t,dossantos2015classification}.
        \item Even with structured representations \citep{weissenborn17}, the final output is still supervised.
        \item Again, supervision does not scale.
        \item Latent variable models (LVMs) are one method for alleviating the need for supervision.
        \item LVMs do not require manual labels, as they instead treat quantities of interest
            as latent random variables and deal with them in a probabilistically principled 
            fashion.
        \end{enumerate}
    \item Recent advances in NLG
        (Need to argue the benefits of generative modeling, is the recent success enough?)
        \begin{enumerate}
        \item Deep generative models, namely LVMs with neural network components,
            have integrated the flexibility of neural networks with the
            inductive biases of graphical models.
        \item Most importantly, efficient techniques for training hard attention 
            that rely on variational inference \citep{deng2018attn}.
            This technique is applicable to sequential LVMs such as hidden Markov models 
            and hidden semi-Markov models (HSMMs).
        \item Use HSMM as signal for training IE.
        \end{enumerate}
    \end{enumerate}
\item Background and Notation
    \begin{enumerate}
    \item Formal notation for elements of the dataset
    \item Define the distribution we would like to learn: $p(z\mid y, x)$.
        $z$ and $x$ are placeholders and will change, but $y$ is always the text.
    \item Link to rotowire example
        (argument is that ACE is made up of ontonotes-like sentences, so all short-form)
    \item Clarify that the scope of posterior inference is very general.
    \end{enumerate}
\item Proposal
    \begin{enumerate}
    \item Outline approach
        \begin{enumerate}
        \item Choose a subset of available data as conditioning,
            and thus it is not modelled.
        \item The joint distribution of the remaining variables,
            both observed and unobserved, will be modelled.
        \end{enumerate}
    \item We present one model as an example, then later demonstrate how the framework
        can handle extensions of the model.
    \item Define generative model: HSMM as in \citep{liang2009semalign},
        and \citep{wiseman2018template}.
        The generative story (a picture would be helpful):
        \begin{enumerate}
        \item Fill in values
        \item Choose alignments
        \item Choose words
        \end{enumerate}
    \item Define IE as the distribution we would like to learn
        \begin{enumerate}
        %\item Align: $p(c\mid\by,\br)$
        \item Values: $p(c,\bv\mid\by,\be,\bt)$ (Just this one)
        %\item ??: $p()$
        \end{enumerate}
    \item We either use the posterior distribution of the conditional model
        or learn an approximation of it.
    \item Argue that the segmental model encourages more coverage
        by adding pairwise dependencies between labelings \citep{liang2009semalign}.
        We will check how much structure in the generative model
        aids information extraction.
    \item Training and Inference
        \begin{enumerate}
        \item As we are dealing with documents of significant length,
            we train with an approximate posterior in order to satisfy memory constraints.
        \item We maximize a lower bound on the log marginal likelihood,
            called the evidence lower bound.
        \end{enumerate}
    \item Extension, \texttt{Types}
        \begin{enumerate}
        \item Introduce new step in generative process
        \item Learn a boolean function that composes predicates applied to existing
            records
        \item The search space is very large, so we must either constrain our model 
            in a very clever way or obtain 
            large amounts of data as any stochastic gradient estimator will have very high variance
        \end{enumerate}
    \item Extensions
        \begin{enumerate}
        \item learn new types as functions of existing ones
        \item learn a randomly initialized embedding of the type
            and a neural network directly to predict the value
        \item let the input distribution be 
        \end{enumerate}
    \item Experiments, evaluation, and expectation
        \begin{enumerate}
        \item Evaluate on Rotowire?
        \item Also on ACE
        \item As corpora may be too large, we might need more hierarchy in the generative model
        \item and also since memory is linear in the length of the sequence, we may have
            to resort to approximate inference. We can optimize a lower bound
            on the marginal likelihood with an approximation of the
            posterior distribution \citep{deng2018attn}.
        \end{enumerate}
    \item Conclusion
        \begin{enumerate}
        \item Please accept!
        \item Recap: Minimal supervision IE systems so that they can scale to
            extracting information for large information networks from large bodies of text.
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

