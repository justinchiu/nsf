I have always recognized effective writing as an extremely difficult ordeal. My long-term research goal is to create tools for understanding and generating text, in order to facilitate the teaching of communication skills. My dream is to build a system that can extract an argument outline from an essay, verify the argumentative structure, then propose improvements to the written text. To that end, my short-term focus is on neural latent variable models for information extraction. Professionally, my long term goal is to remain in academia and develop technology that helps inform and empower our country's citizens through natural language processing (NLP).

I became interested in NLP during my undergraduate studies at the University of Pennsylvania, where I took and subsequently served as a teaching assistant for the machine translation class taught by Prof. Chris Callison-Burch. The class had an emphasis on statistical machine translation, which immediately captivated my interest. As a research engineer at Facebook AI Research, I continued to work on machine translation and developed a strong familiarity with neural networks. After two years at Facebook, I resumed my studies when I joined Prof. Rush's NLP lab at Harvard University as a graduate student. The motivation behind furthering my education was a desire to incorporate a more principled approach to modeling than only neural networks afforded. Graphical models provide a principled method for answering probabilistic queries, which is necessary for reasoning about the unobserved elements of language. My labmates and I recently demonstrated that combining both graphical models and neural networks for the task of translation yields performance improvements, and also allows us to inspect the model's internal beliefs. Our paper, "Latent Alignment and Variational Attention," was accepted at NeurIPS 2018.

Recently, my focus has shifted from translation to information extraction. After working on latent variable models for translation, I realized the potential a similar model would have for information extraction. A model similar to the one we proposed for translation could be used to pull information out of text. Given a powerful information extraction model that is able to understand text, the remaining step towards a writing aid would be the creation of a verifiable argumentation structure. Such a structure would allow a verifier to determine whether the argument logically entails the desired conclusion. The final product would aid developing writers in forming and structuring their arguments, an important part of developing effective communication skills.

The NDSEG fellowship would afford me the freedom to devote my research to the application of generative models to the problem of information extraction, with the hope of moving towards models that can understand and improve upon our own writing.


