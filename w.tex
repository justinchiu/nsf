\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\begin{comment}
Spectrum from hard attention => hard segmental => HSMM for generative model, coverage / recall
Should we go into HSMM??? or is hard segmental attention enough

composition function = categorical over two entries of x

structured attention for IE posterior?
\end{comment}

\paragraph{Keywords}
information networks, natural language processing, information extraction,
latent variable models

\paragraph{Relevance to BAA}
As information networks get larger and more complex,
acquiring explicit supervision for the training of information extraction systems
becomes extremely expensive.
Both the representation of the information in the knowledge base
as well as the extraction process itself must be hand-designed.
In this proposal we present a framework towards automating the
training of information extraction systems with minimal supervision.

\paragraph{Background}
Natural language processing can largely be decomposed into two separate but
closely related tasks: natural language understanding (NLU) and natural language
generation (NLG).
We argue that the two tasks are complementary,
and capitalize on their duality by proposing a method to train a NLU system
without direct supervision.
More specifically, we utilize the latent variable model framework to train an information
extraction system using the performance of a generative model of text as learning signal.
Intuitively, we want an information extraction system to extract information that
best explains the given text.
We focus on the task of summarizing a table of data,
which we refer to as Data-to-Text (D2T).

\paragraph{Why is D2T important?}
\begin{comment}
\item Convince readers that this is a good information extraction task,
similar to semantic parsing / instruction following?
\item Highlight: Closed domain
\item TODO: probably need to rewrite this, rethink framing
\end{comment}
Unlike tasks such as translation or summarization where one would
need to first understand a source passage then generate a target passage,
D2T isolates the task of NLU from NLG by introducing an intermediate representation in
the form of structured data.
This makes the task similar to semantic parsing, program induction, and instruction following,
except for the lack of compositionality in D2T's intermediate representation.
Whereas the other tasks assume an executable intermediate representation,
D2T has a latent representation that takes the form of alignments from 
words in the summary to their respective referent data.
This allows D2T to serve as a useful benchmark for conditional text generation
because of the reduced complexity.
In addition to establishing a convenient representation,
D2T allows us to assume relatively safely that the text is generated conditioned only on
functions of the data table.
This assumption is central to the task as it allows
practitioners to define heuristics both for providing weak supervision to an
information extraction system as well as evaluating a summary's fidelity
to the underlying data.
% initial analysis shows this assumption holds x% of the time.

% include brief review here?

%Modern neural systems have demonstrated state-of-the-art performance on short-form
%text generation tasks, but the performance seemingly fails to generalize to long-form generation.
%D2T is a task that isolates performance on long-form generation by disentangling 
%generation quality from input representation.

\paragraph{Rotowire}
\begin{enumerate}
\item Motivating extraction example
\item Transition towards more formal definitions so that we can define the tasks
\item TODO: Can maybe replace this whole section with a picture.
\end{enumerate}
We provide a concrete example from a D2T dataset:
In this proposal we focus on the recently proposed
the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games aligned with the respective
box scores of those games.
We consider a box score to be a collection or set of facts,
where a fact is a tuple of an entity, relation type, and value.
For example, consider the collection:
a collection that consists of a single fact, (entity = Jeremy Lin, type = POINTS, value = 19),
and a simple statement ``Jeremy Lin scored 19 points''.
The relation type is the label in the box score, for example POINTS, REBOUNDS, etc.
In order to automatically evaluate this summary,
thanks to our assumption the summary was generated from only the data,
we are able to use an information extraction system (IE) to extract entity-value pairs
from text,
predict the type of the relation between the entity and value,
and compare resulting tuple (entity, type, value) to the given collection of facts.

\paragraph{Our proposal}
Our goal is to maximize the \underline{coverage} of the information extraction system,
which we define as the number of words contained within segments of text that are correctly aligned to data,
while minimizing the amount of supervision needed.
We propose to learn an information extraction (IE) system as the approximate posterior distribution over 
alignments from segments of text to their generating data.
We also propose to view the given data as incomplete
and learn boolean-valued functions of the data as a step
towards representation learning.

\paragraph{Problem Definition}
\begin{enumerate}
\item Define notation
\item Define generation, mention sam and puduppully work.
\item Define extraction and its subtasks, mention regina and percy's work.
\item TODO: maybe combine align and values? Although I prefer to keep them separate
since values will probably turn into fill in the away team's values given home team's
\end{enumerate}
Rotowire consists of aligned box score data and basketball game summaries
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single data and summary as $\br,\by$, omitting the superscript.
Each data $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
We refer collectively to all the entities, types, and values in a given data $\br$ as
$\be,\bt,\bv$ respectively.
Each summary $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens that makes up the
text of the game description.

For generation, the goal is to learn a conditional model $p(\by\mid\br)$ of the text given the data.
This is simple to evaluate, as we can use the log-likelihood of a given summary under our model
as a measure of performance.
In \citet{wiseman2017d2t} model takes the form of a conditional language model
that can copy values directly from records in $\br$.
Subsequent work in \citet{puduppully2018contentselection} decomposed the distribution 
$p(\by\mid\br) = \sum_{\bc}p(\by\mid\bc)p(\bc\mid\br)$
by introducing a content plan $\bc$, which is a sequence of records drawn from $\br$.
This was also previously implemented in prior work \citep{liang2009semalign},
which modelled the text generation process through a hierarchical hidden semi-Markov model.
% Definitely need a figure here

We divide the information extraction task into three subtasks.
Before outlining the tasks, we propose two measures of performance through which
to evaluate an unsupervised information extraction system.
The first is how well the information extracted from a summary allows a 
generative model to reconstruct the summary measured by the likelihood
of the summary given the extracted information.
We refer to this as \underline{reconstruction}.
The second is \underline{coverage}, 
which we define as the number of words contained in segments
that are aligned to a record in $\br$.
(Do I need to argue why these are useful?
And also how the tasks aim at increasing them by weakening assumptions
or constraining model flexibility compared to previous work)

The first task (\textsc{align}) is to align segments of text to the records that generated them.
This is similar to learning a content plan $\bc\mid\br$ as in \citet{puduppully2018contentselection},
however we are interested in the \textbf{posterior} distribution $p(\bc\mid\br,\by)$ of
the content plan $\bc$ after observing the text $\by$.
\citet{liang2009semalign} utilize the fact that they define a model in which posterior inference is tractable,
however tractability does not hold once the latent distribution becomes autoregressive.
% Elaborate
In \citet{wiseman2017d2t} and subsequently in \citet{puduppully2018contentselection} this was accomplished by 
separately training a classifier to predict the type $t$ of an entity $e$ and value $v$ pair within a sentence.
The entity and value are extracted heuristically by checking exact string matches within $\be$ and $\bv$,
and the supervision over $t$ is obtained through the following function \citep{wiseman2017d2t}:
$\text{findType}(\hat{e},\hat{v}) = \set{ r.t :x\in\br, r.e = \hat{e}, x.r = \hat{v}}$.
However, this limits alignments exclusively to entities and values explicitly in $\br$.
We would like to align whole segments of text in order to increase the coverage
of our information extraction system.


The second task (\textsc{values}) is to reconstruct values $v$ in the table $\br$.
This is implemented on top of task (\textsc{align}).
In particular, we want to learn $p(\bv \mid \by,\bc,\be,\bt)$,
the distribution over all values given the summary, the content plan, all entities, and all types.

The third task (\textsc{functions}) is the most ambitious.
In order to demonstrate the flexibility of the framework,
we propose a method to further learn functions of $\br$ in an unsupervised manner.
(TODO)

\paragraph{Model}
We begin by defining a model for (\textsc{align})
and proceed to (\textsc{values}) and (\textsc{functions}).

% Generative model
Our generative model factors into the
likelihood and prior: $p(\by,\bc\mid\br)=p(\by\mid\bc)p(\bc\mid\br)$.
Our likelihood is given by a conditional language model
with a copy mechanism as in \citet{gulcehre2016cc,wiseman2017d2t}
in addition to monotonic attention \citep{yu2016ssnt,wiseman2018template}.
The prior is simply an autoregressive model over records.
(This is the simplest baseline aside from HSMM, can make things much more complicated)

% IE model
Our initial IE model for (\textsc{align}) is given by $p(\bc\mid\by,\br)$,
which includes both a segmentation of the summary as well as the alignments.
Initially we parameterize $p(\bc\mid\by,\br)=\prod_tp(c_i\mid\by,\br)$ as
a fully factored distribution over alignments.
(Structured attention for pairwise potentials later)

TODO: values, functions

\paragraph{Learning and Inference}
\begin{enumerate}
\item Question: Do I need to motivate approximate inference?
I could also argue that rather than computing the posterior exactly,
at test time using the approximation directly can be more efficient,
especially if it is fully factored
\item Need to figure out if this is the right place for this
\item Describe how to incorporate the weak supervision through posterior constraints
\end{enumerate}
A conditional latent variable model $p(\by,\bz\mid\bx)$ models an
observed $\by$ as well as an unobserved $\bz$ conditioned on $\bx$.
When fitting such a model, we would like to maximize the evidence
$p(\by\mid\bx)=\sum_\bz p(\by,\bz\mid\bx)$ which marginalizes over
the latent $\bz$.
Depending on the structure of $p(\by,\bz\mid\bx)$,
the marginalization procedure may be intractable to perform exactly.
For example, this is the case with an autoregressive model for the latent
variable $p(z_t\mid \bz_{<t})$, where variable elimination's runtime would be
exponential in the length $|\bz|$.
This also precludes tractable posterior inference, i.e. $p(\bz\mid\by,\bx)$,
since by Bayes' Rule we have $p(\bz\mid\by,\bx)=p(\by,\bz\mid\bx)/p(\by\mid\bx)$
which requires evaluating the intractable sum in the evidence.
Therefore we resort to learning an approximation of the posterior through the variational principle:
the best approximation within a family of distributions is the one with minimal KL-divergence
to the model's posterior.

For task (\textsc{align}), we have the fully observed summary $\by$,
the unobserved content plan $\bz=\bc$, and all records as conditioning $\bx=\br$.
For task (\textsc{values}), we again have the observed summary $\by$,
but we pretend the values are unobserved $\bz=\set{\bc,\bv}$, and
use the rest of the records as conditioning $\bx=\set{\be,\bt}$.

TODO: functions, and then cover KLpost = -ELBO + evidence
\begin{comment}
\section{OLD}

\paragraph{Research Question}
%We can train end-to-end modular systems,
%so how much modularity and supervision can we provide and does it help the model?
As research on D2T has embraced neural systems and end-to-end training resulting
in very unstructured models, what is the benefit of imbuing more structure, namely in the
form of a hierarchical segmental model?
%As we add more complexity to our graphical model, what is the benefit of a latent variable model
%versus a soft approximation?
Using a hierarchical HSMM allows us to model
sentences in a modular and controllable way;
we can control the number of records referred to in a sentence and 
the structure of the sentence itself.
The formulation as a latent variable model allows us to enforce strict constraints on
the modeled characteristics (namely the number of reference records and sentence structure)
during inference, allowing a user to intervene during the text generation process.

NEW: We propose to learn an information extraction system jointly with a 
generative model of text.
By doing so we incorporate recent advances in generative modeling
in order to improve the information extraction model.

\begin{wrapfigure}[22]{r}{0.45\textwidth}
\centering
\begin{subfigure}[]{0.4\textwidth}
\centering
\begin{tikzpicture}
\node(x)[obs]{$\bx$};
\node(s)[latent, right =of x]{$\bs$};
\node(r)[latent, right =of s]{$\br$};
\node(y)[latent, right =of r]{$\by$};
\edge {x} {s};
\edge [bend right=30] {x} {r};
\edge {s} {r};
\edge [bend right=30] {s} {y};
\edge {r} {y};
\end{tikzpicture}
\label{tikz:simple}
\caption{}
\end{subfigure}

\begin{subfigure}[]{0.4\textwidth}
\centering
\begin{tikzpicture}
\node(s0)[latent]{$s_0$};
\node(r0)[latent, below =of s0]{$r_0$};
\node(r0d)[latent, below right =4mm and 3mm of r0]{$r_0^D$};
\node(y0)[latent, below =of r0]{$y_0$};
\node(s1)[latent, right =of s0]{$s_1$};
\node(r1)[latent, below =of s1]{$r_1$};
\node(r1d)[latent, below right =4mm and 3mm of r1]{$r_1^D$};
\node(y1)[latent, below =of r1]{$y_1$};
\node(s2)[latent, right =of s1]{$s_2$};
\node(r2)[latent, below =of s2]{$r_2$};
\node(r2d)[latent, below right =4mm and 3mm of r2]{$r_2^D$};
\node(y2)[latent, below =of r2]{$y_2$};

\edge {s0} {r0};
\edge [bend left=15] {s0} {r0d};
\edge [bend right=35] {s0} {y0};
\edge {r0} {y0};
\edge {r0} {r0d};

\edge {s1} {r1};
\edge [bend left=15] {s1} {r1d};
\edge [bend right=35] {s1} {y1};
\edge {r1} {y1};
\edge {r1} {r1d};

\edge {s2} {r2};
\edge [bend left=15] {s2} {r2d};
\edge [bend right=35] {s2} {y2};
\edge {r2} {y2};
\edge {r2} {r2d};

\edge {s0} {s1};
\edge {r0} {r1};
\edge {y0} {y1};
\edge {r0d} {r1};

\edge {r0d} {r1d};
\edge [bend left=15] {y0} {s1};

\edge {s1} {s2};
\edge {r1} {r2};
\edge [bend right=25] {y0} {y2};
\edge {y1} {y2};
\edge {r1d} {r2};

\edge {r1d} {r2d};
\edge [bend left=15] {y1} {s2};


\end{tikzpicture}
\label{tikz:full}
\caption{}
\end{subfigure}
\label{fig:pgm}
\caption{
(a) is a simplified graphical representation of the model
that ignores the temporal dependencies.
$\bx$ is the input data, $\bs$ the style, $\br$ the content,
and $\by$ the words of the summary.
(b) is the full graphical model.
We leave out conditioning on $\bx$ for brevity.
The new variables $r_i^D$ represent the duration a record is used.
}
\end{wrapfigure}
\paragraph{Argument for LVM}
\begin{enumerate}
\item The fully neural approach in \citep{puduppully2018contentselection} takes the output of
an information extraction system as the ground truth and trains a content planning model with
that assumption.
Any mistakes in the information extraction system will be replicated in the final model.
%In addition, the information extraction is trained via a heuristic alignment
%procedure that aligns words that appear in the database to records containing those words. 
%However, we argue that this particular procedure results in a compounding of mistakes.
% should use more precise notation here
Thus we argue that training the information extraction system by incorporating it
into an approximate posterior will improve performance as measured by
the likelihood of the data under our model.
\item As demonstrated in \citep{liang2009semalign},
a structured LVM results in qualitatively better segmentations than an unstructured one.
The latent variable formulation allows us to impose different inductive biases than a 
purely neural model.
For example, \citep{liang2009semalign} enforce monotonic alignments between text and records by using a 
hidden semi-markov model (HSMM).
\item Convincing templates from prior work \citep{wiseman2018template} indicate that the separation of
style and content is plausible.
\item Explicitly modeling characteristics such as the number of records in a given
sentence is possible in a latent variable model such as a HSMM.
\item Long-form text is HIGHLY multimodal, as there are many valid permutations of content. 
However, conditioning on content should help greatly with that multimodality.
\end{enumerate}

\paragraph{Approach and Methods}
% Proposal? Add more structure / modularity, principled inference...?
% Thesis: structure will help with controllability and LVM formulation
% with generalization as it presents a different inductive bias?
We would like to unify works from several directions:
\begin{enumerate}
\item Structure away content modeling through templates
\citep{sauper2009wiki,wiseman2018template}
\item Content planning and incorporating weak supervision
\citep{puduppully2018contentselection}
\item Efficient training methods for discrete LVMs \citep{deng2018vattn}
\end{enumerate}
% Elaborate on each of those points
We propose to model the conditional distribution of a summary given data as a 
hierarchical HSMM, as in \citep{liang2009semalign}.
Ignoring the temporal aspect of the model, we define a simplified model
with the following parts:
\begin{enumerate}
\item The given structured data $\bx$ 
\item Each sentence is generated using a style $s_i$
\item The content in each sentence $\br_i$ is chosen from $\bx$
after choosing a style $s_i$
\item The words in the summary $y_i$ are chosen conditioning on
the current content from $r_i$ as well as the style $c_i$
\end{enumerate}
We also present a more detailed model:
% Caveat: we don't want r_t to depend on r_{t-1} if starting a new sentence.
\begin{enumerate}
\item The style features $s_t\mid y_{t-1},s_{t-1},\bx\sim\Cat()$
\item The record choices $r_t\mid r_{t-1}^D,r_{t-1},s_t,\bx\sim\Cat()$
\item The record duration is given by 
$r_t^D\mid r_{t-1}^D=0,r_t\sim\Unif(1,\ldots,L)$
where $L$ is the max segment length and $r_t^D\mid r_{t-1}^D=x = x-1$

\item The words in the summary $y_t\mid y_{<t},r_t,s_t\sim\Cat()$
\end{enumerate}
See Figure~\ref{fig:pgm} for a simplified graphical depiction of the model.
We include a fully-specified slice of the time-series model as well,
but do not elaborate on it.

% too expensive to align to records since there may be many of them
As defined, the model would be very expensive to train via maximum marginal likelihood
training. We therefore propose to use the method proposed in \citep{deng2018vattn}
in order to make training tractable: namely define a variational approximation to the
posterior and use REINFORCE to maximize a lower bound on the marginal likelihood.
We also propose to incorporate intermediate weak supervision in the form of
heuristic record-to-text alignments.

% Should I explain why I propose a LVM versus a fully discriminative model?
% 
\paragraph{Intellectual Merit}
We hope to demonstrate the benefits of a hierarchical model in the task of text generation.
We propose a structure-aware content model similar to
work in \citep{sauper2009wiki,wiseman2017d2t,liang2009semalign}
that incorporates the computational advances in \citep{deng2018vattn}
in order to scale training.

\paragraph{Broader Impact}
The task of D2T itself is important, rather than simply a compromise between
short and long-form generation tasks.
Given a complicated set of structured data with possibly many records
which are irrelevant, an ideal D2T system would perform information triage by 
picking salient records then organize those records into prose.
This would be much easier to understand for someone unfamiliar with the
particular structure of the data and would aid in the dissemination of 
technological understanding.
With an interpretable LVM the user would also be able to examine the aligned
records that generated the summary, allowing the user to learn to read
the data by following the system's example.
% Stretch

\section{Outline}
\begin{enumerate}
\item Background
\begin{enumerate}
\item Breakdown of NLP into NLU and NLG
\item Importance of D2T, where we can learn a system to jointly perform NLU and NLG
\begin{enumerate}
\item Given the recent success with short-form generation in translation,
we can claim that we are good at generating text with strong conditioning.
In most cases, the source sentence provides all the information the model needs to translate.
However, for long-form translation current neural models do to not perform well.
%We hypothesize that this is because 
\item A compromise between short-form and long-form conditional generation  
\item Easily evaluable benchmark due to limited use of external knowledge
\item Convenient representation of input allows us to isolate progress
\item 
progress on generation metrics
\end{enumerate}
\item Introduce Rotowire \citep{wiseman2017d2t}
\begin{enumerate}
\item A summary of a basketball game is modelled conditioned on the box score
associated with that game.
\item The box score consists of a list of records associating an entity with a value.
Each record has a type which denotes the relationship between the entity and value
contained in the record,
for example: (type = POINTS, entity = Jeremy Lin, value = 19).
\end{enumerate}
\item Desiderata of Text Gen Systems, and how they are measured
\begin{enumerate}
\item Readability: How easy is it for a human to read the generated summary?
Refers to fluency and grammaticality, and is measured by BLEU as the two have been
shown to be correlated.
\item Informational Adequacy: How well does the model recreate the informational content
of a human-generated summary?
Measured through metrics concerning the selection and ordering of records.
% Discuss difficulties
\item Fidelity to conditioning: 
A lack of fidelity occurs if the model either ignores the data input and hallucinates
a value through the language model or if the model misaligns a record to a given segment of text.
\item Controllability:
Although a precise definition is available for dynamical systems,
we have yet to formalize a notion of controllability for NLG.
We loosely define controllability as the ability to apply constraints during inference.
% Maybe delta BLEU when using an oracle plan?
\end{enumerate}
\item Current trends addressing desiderata
\begin{enumerate}
\item Initially research focused largely on readability,
and improved BLEU scores through the use of conditional neural language models.
Compared to template-based methods, summaries were more fluent and natural.
However, by removing structure from the model and relying on the flexibility of neural networks
informational adequacy, fidelity, and controllability were sacrificed \citep{wiseman2017d2t}.
\item Subsequent work re-introduced the explicit modeling of content
\citep{puduppully2018contentselection}, which improved informational adequacy and
fidelity to conditioning through a latent content plan.
Although not entirely explored in the aforementioned work,
modeling the content plan also affords a degree of controllability.
By using oracle content plans extracted from the data,
they manage to obtain an upper bound on BLEU under their model.
\end{enumerate}
\end{enumerate}
\item Research Question
\begin{enumerate}
\item As research on D2T has embraced neural systems and end-to-end training
resulting in very unstructured models,
what is the benefit of imbuing more structure,
namely in the form of a hierarchical segmental model?
For the task of data-to-text, using a hierarchical HSMM allows us to model sentences
in a modular and controllable way;
we can control the number of records referred to in a sentence and the structure
of the sentence itself.
\end{enumerate}
\item Argument for a LVM:
Although the argument for modularity applies equally to models expressible as
neural networks as well as latent variable models, LVMs allow us to:
\begin{enumerate}
\item Learn an information extraction system jointly with the generative model.
\item Encode different inductive biases than neural models due to the ability to make hard decisions
and enforce structure.
\item Utilize indirect supervision over record alignments in a principled way.
\end{enumerate}
\item Approach and Methods
\begin{enumerate}
\item We propose a structure-aware content selection \citep{sauper2009wiki} model
as a hierarchical HSMM \citep{wiseman2018template}.
\item We incorporate weak supervision for the content planning model and
information extraction model through posterior constraints during sampling.
\item Our conditional generative model takes the form of a hierarchical
HSMM, where a sentence structure is first chosen, then the resulting records, and finally
the text to describe those records in accordance with the structure of the sentence.
\item At training time inference is quite difficult due to the large number of records,
so we resort to variational inference as in \citep{deng2018vattn}. 
\end{enumerate}
\item Extensions:
\begin{enumerate}
\item Fully generative model to reason about uncertainty over table values.
\item Fully generative model can include a model of table completion,
where unobserved values are functions of existing elements.
Maybe view this as a form of very limited reasoning.
\end{enumerate}
\item Relevance to BAA:
\begin{enumerate}
\item The thrust of information networks is concerned with representing
and acquiring knowledge graphs from various streams of data.
\item We propose to learn an information extraction system that is trained 
jointly with a generative model of the text.
\end{enumerate}
\end{enumerate}

REINFORCE HSMM training: use a soft HMM as baseline.
\end{comment}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

