\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\begin{comment}
Spectrum from hard attention => hard segmental => HSMM for generative model, coverage / recall
Should we go into HSMM??? or is hard segmental attention enough

composition function = categorical over two entries of x

structured attention for IE posterior?
\end{comment}

\paragraph{Keywords}
information networks, natural language processing, information extraction,
latent variable models

\paragraph{Relevance to BAA}
As information networks get larger and more complex,
acquiring explicit supervision for the training of information extraction systems
becomes extremely expensive.
Both the representation of the information in the knowledge base
as well as the extraction process itself must be hand-designed.
In this proposal we present a framework towards automating the
training of information extraction systems with minimal supervision.

\paragraph{Background}
Natural language processing can largely be decomposed into two separate but
closely related tasks: natural language understanding (NLU) and natural language
generation (NLG).
We argue that the two tasks are complementary,
and capitalize on their duality by proposing a method to train a NLU system
without direct supervision.
More specifically, we utilize the latent variable model framework to train an information
extraction system using the performance of a generative model of text as learning signal.
Intuitively, we want an information extraction system to extract information that
best explains the given text.
We focus on the task of summarizing a table of data,
which we refer to as Data-to-Text (D2T).

\paragraph{Why is D2T important?}
\begin{comment}
\item Convince readers that this is a good information extraction task,
similar to semantic parsing / instruction following?
\item Highlight: Closed domain
\item TODO: probably need to rewrite this, rethink framing
\end{comment}
Unlike tasks such as translation or summarization where one would
need to first understand a source passage then generate a target passage,
D2T isolates the task of NLU from NLG by introducing an intermediate representation in
the form of structured data.
This makes the task similar to semantic parsing, program induction, and instruction following,
except for the lack of compositionality in D2T's intermediate representation.
Whereas the other tasks assume an executable intermediate representation,
D2T has a latent representation that takes the form of alignments from 
words in the summary to their respective referent data.
This allows D2T to serve as a useful benchmark for conditional text generation
because of the reduced complexity.
In addition to establishing a convenient representation,
D2T allows us to assume relatively safely that the text is generated conditioned only on
functions of the data table.
This assumption is central to the task as it allows
practitioners to define heuristics both for providing weak supervision to an
information extraction system as well as evaluating a summary's fidelity
to the underlying data.
% initial analysis shows this assumption holds x% of the time.

% include brief review here?

%Modern neural systems have demonstrated state-of-the-art performance on short-form
%text generation tasks, but the performance seemingly fails to generalize to long-form generation.
%D2T is a task that isolates performance on long-form generation by disentangling 
%generation quality from input representation.

\paragraph{Rotowire}
\begin{comment}
\item Motivating extraction example
\item Transition towards more formal definitions so that we can define the tasks
\item TODO: Can maybe replace this whole section with a picture.
\end{comment}
We provide a concrete example from a D2T dataset:
In this proposal we focus on the recently proposed
the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games aligned with the respective
box scores of those games.
We consider a box score to be a collection or set of facts,
where a fact is a tuple of an entity, relation type, and value.
For example, consider the collection:
a collection that consists of a single fact, (entity = Jeremy Lin, type = POINTS, value = 19),
and a simple statement ``Jeremy Lin scored 19 points''.
The relation type is the label in the box score, for example POINTS, REBOUNDS, etc.
In order to automatically evaluate this summary,
thanks to our assumption the summary was generated from only the data,
we are able to use an information extraction system (IE) to extract entity-value pairs
from text,
predict the type of the relation between the entity and value,
and compare resulting tuple (entity, type, value) to the given collection of facts.

\paragraph{Our proposal}
Our goal is to maximize the \underline{coverage} of the information extraction system,
which we define as the number of words contained within segments of text that are correctly aligned to data,
while minimizing the amount of supervision needed.
We propose to learn an information extraction (IE) system as the approximate posterior distribution over 
alignments from segments of text to their generating data.
We also propose to view the given data as incomplete
and learn boolean-valued functions of the data as a step
towards representation learning.

\paragraph{Problem Definition}
\begin{comment}
\item Define notation
\item Define generation, mention sam and puduppully work.
\item Define extraction and its subtasks, mention regina and percy's work.
\item TODO: maybe combine align and values? Although I prefer to keep them separate
since values will probably turn into fill in the away team's values given home team's
\end{comment}
Rotowire consists of aligned box score data and basketball game summaries
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single data and summary as $\br,\by$, omitting the superscript.
Each data $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
We refer collectively to all the entities, types, and values in a given data $\br$ as
$\be,\bt,\bv$ respectively.
Each summary $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens that makes up the
text of the game description.

For generation, the goal is to learn a conditional model $p(\by\mid\br)$ of the text given the data.
This is simple to evaluate, as we can use the log-likelihood of a given summary under our model
as a measure of performance.
In \citet{wiseman2017d2t} the model takes the form of a conditional language model
that can copy values directly from records in $\br$.
Subsequent work in \citet{puduppully2018contentselection} decomposed the distribution 
$p(\by\mid\br) = \sum_{\bc}p(\by\mid\bc)p(\bc\mid\br)$
by introducing a content plan $\bc$, which is a sequence of records drawn from $\br$.
This was also previously implemented in prior work \citep{liang2009semalign},
which modelled the text generation process through a hierarchical hidden semi-Markov model.
% Definitely need a figure here

We divide the information extraction task into three subtasks.
Before outlining the tasks, we propose two measures of performance through which
to evaluate an unsupervised information extraction system.
The first is how well the information extracted from a summary allows a 
generative model to reconstruct the summary measured by the likelihood
of the summary given the extracted information.
We refer to this as \underline{reconstruction}.
The second is \underline{coverage}, 
which we define as the number of words contained in segments
that are aligned to a record in $\br$.
(Do I need to argue why these are useful?
And also how the tasks aim at increasing them by weakening assumptions
or constraining model flexibility compared to previous work)

The first task (\textsc{align}) is to align segments of text to the records that generated them.
This is similar to learning a content plan $\bc\mid\br$ as in \citet{puduppully2018contentselection},
however we are interested in the \textbf{posterior} distribution $p(\bc\mid\br,\by)$ of
the content plan $\bc$ after observing the text $\by$.
\citet{liang2009semalign} utilize the fact that they define a model in which posterior inference is tractable,
however tractability does not hold once the latent distribution becomes autoregressive.
% Elaborate
In \citet{wiseman2017d2t} and subsequently in \citet{puduppully2018contentselection} this was accomplished by 
separately training a classifier to predict the type $t$ of an entity $e$ and value $v$ pair within a sentence.
The entity and value are extracted heuristically by checking exact string matches within $\be$ and $\bv$,
and the supervision over $t$ is obtained through the following function \citep{wiseman2017d2t}:
$\text{findType}(\hat{e},\hat{v}) = \set{ r.t :x\in\br, r.e = \hat{e}, x.r = \hat{v}}$.
However, this limits alignments exclusively to entities and values explicitly in $\br$.
We would like to align whole segments of text in order to increase the coverage
of our information extraction system.


The second task (\textsc{values}) is to reconstruct values $v$ in the table $\br$.
This is implemented on top of task (\textsc{align}).
In particular, we want to learn $p(\bv \mid \by,\bc,\be,\bt)$,
the distribution over all values given the summary, the content plan, all entities, and all types.

The third task (\textsc{functions}) is the most ambitious.
In order to demonstrate the flexibility of the framework,
we propose a method to further learn functions of $\br$ in an unsupervised manner.
(TODO)

\paragraph{Model}
We begin by defining a model for (\textsc{align})
and proceed to (\textsc{values}) and (\textsc{functions}).

% Generative model
Our generative model factors into the
likelihood and prior: $p(\by,\bc\mid\br)=p(\by\mid\bc)p(\bc\mid\br)$.
Our likelihood $p(\by\mid\bc)$ is given by a conditional neural language model
with a copy mechanism as in \citet{gulcehre2016cc,wiseman2017d2t}
in addition to monotonic attention \citep{yu2016ssnt,wiseman2018template}.
The prior $p(\bc\mid\br)$ is an autoregressive model over records
parameterized with an LSTM.
As we are primarily interested in posterior inference, the performance of the prior is
not the most important aspect of the model.
In fact, we will see in the next section that the prior serves to regularize
our approximate posterior.
(This is the simplest baseline aside from HSMM, can include 
$p(\by,\bc\mid\br)=\prod_tp(y_t\mid\by_{<t},c_t)p(c_t\mid\bc_{<t},\by_{<t},\br)$
if necessary)

% IE model
Our initial IE model for (\textsc{align}) is given by $q(\bc\mid\by,\br)$,
which includes both a segmentation of the summary as well as the alignments.
Note that we denote the distribution using $q$ since under the generative model
$p(\bc\mid\by,\br)$ is well-defined as the posterior distribution of alignments
given a summary and records.
Initially we parameterize the approximate posterior
$q(\bc\mid\by,\br)=\prod_tq(c_i\mid\by,\br)$
as a fully factored distribution over alignments.
(Structured attention for pairwise potentials later,
since only has node potentials for now.
This is motivated by \underline{coverage})
Each $q(c_i\mid\by,\br)$ is parameterized by the output
of a BLSTM run at the sentence level.
(TODO: values, functions)

\paragraph{Learning and Inference}
\begin{comment}
\item Question: Do I need to motivate approximate inference?
I could also argue that rather than computing the posterior exactly,
at test time using the approximation directly can be more efficient,
especially if it is fully factored
\item Need to figure out if this is the right place for this
\item Describe how to incorporate the weak supervision through posterior constraints
\end{comment}
A latent variable model $p(\by,\bz\mid\bx)$ models an
observed $\by$ as well as an unobserved $\bz$ conditioned on $\bx$.
When fitting such a model, we would like to maximize the evidence
$p(\by\mid\bx)=\sum_\bz p(\by,\bz\mid\bx)$ which marginalizes over
the latent $\bz$.
Depending on the structure of $p(\by,\bz\mid\bx)$,
the marginalization procedure may be intractable to perform exactly.
For example, this is the case with an autoregressive model for the latent
variable $p(z_t\mid \bz_{<t})$, where variable elimination's runtime would be
exponential in the length $|\bz|$.
This also precludes tractable posterior inference, i.e. $p(\bz\mid\by,\bx)$,
since by Bayes' Rule we have $p(\bz\mid\by,\bx)=p(\by,\bz\mid\bx)/p(\by\mid\bx)$
which requires evaluating the intractable sum in the evidence $p(\by\mid\bx)$.
Therefore we resort to learning an approximation of the posterior through the variational principle:
the best approximation within a family of distributions is the one with minimal KL-divergence
to the model's posterior.
The KL between the approximate posterior and true posterior is still intractable to minimize exactly,
so we instead maximize the evidence lower bound ($\mathcal{L}_q$), which is the evidence minus the posterior KL:
\begin{linenomath*}
\begin{align}
\mathcal{L}_q
&= \underbrace{\log p(\by\mid\bx)}_{\textrm{Evidence}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx)||p(\bz\mid\by,\bx)]}_{\textrm{Posterior KL}}\\
&= \underbrace{\Es{\bz\sim q(\bz\mid\by,\bx)}{\log p(\by\mid\bz,\bx)}}_{\textrm{Reconstruction}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx) || p(\bz\mid\bx)]}_{\textrm{Prior KL}}.
\end{align}
\end{linenomath*}
Were it not for local extrema,
maximizing this quantity would maximize the evidence and minimize the posterior KL jointly.
Notice that in (2), the objective we use for training both the generative model and IE system,
all expectations are taken with respect to the IE system or approximate posterior $q(\by\mid\bz,\bx)$.
(TODO: training procedure via REINFORCE + control variate, posterior constraints for incorporating 
information from findType function)

For task (\textsc{align}), we have the fully observed summary $\by$,
the unobserved content plan $\bz=\bc$, and all records as conditioning $\bx=\br$.
For task (\textsc{values}), we again have the observed summary $\by$,
but we pretend the values are unobserved $\bz=\set{\bc,\bv}$, and
use the rest of the records as conditioning $\bx=\set{\be,\bt}$.
(TODO: functions)

\newpage
\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item What are information networks? Characterized by large graphs and voluminous data.
        \item How can we learn nodes and connections?
        \item Representation is key
        \item As they scale, acquiring explicit supervision 
            for training an IE system becomes prohibitively expensive.
        \item Propose a method towards training IE systems
            with minimal supervision.
        \end{enumerate}
    \item Decomposition of NLP into NLU and NLG
        \begin{enumerate}
        \item Leverage the duality between NLU and NLG
            to train an NLU system end-to-end without direct supervision
        \item We want to train an IE system
            that explains the given text.
        \end{enumerate}
    \item Recent advances in NLG and stagnation in NLU
        \begin{enumerate}
        \item Incorporation of neural networks in LVMs (templates, attention)
        \item However, most still frame NLU as a classification problem.
            Incorporate a degree of structure (Guillaume CRF BLSTM),
            but still rely on strong supervision.
        \end{enumerate}
    \end{enumerate}
\item Background
    \begin{enumerate}
    \item Formal problem definition? Or just intuition? (Need to ask Sasha)
    \item Motivate LVMs
    \end{enumerate}
\item Problem Definition
    \begin{enumerate}
    \item Setup notation
    \item Link notation to an example?
    \item Define tasks as the distributions we would like to learn
    \end{enumerate}
\item Proposal (All other items below are subsections?)
    \begin{enumerate}
    \item Learn the IE system by using it as the approximate posterior
        of a LVM for summarization in order to leverage advances in NLG.
    \end{enumerate}
\item Model
    \begin{enumerate}
    \item 
    \end{enumerate}
\item Inference
    \begin{enumerate}
    \item 
    \end{enumerate}
\item Justification
    \begin{enumerate}
    \item 
    \end{enumerate}
\item Inference
    \begin{enumerate}
    \item 
    \end{enumerate}
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

