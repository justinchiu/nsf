\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bt}{\mathbf{t}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}

\usepackage[displaymath,mathlines]{lineno}
\linenumbers

\begin{document}
\lhead{Justin Chiu}
\chead{2018 NDSEG Application}
\rhead{Research Proposal}

\begin{center}
%\textbf{Exploiting the Duality of Natural Language Generation and Understanding}
\textbf{Information Extraction with Weak Supervision}
\end{center}

\begin{comment}
Spectrum from hard attention => hard segmental => HSMM for generative model, coverage / recall
Should we go into HSMM??? or is hard segmental attention enough

composition function = categorical over two entries of x

structured attention for IE posterior?
\end{comment}

\paragraph{Keywords}
information networks, natural language processing, information extraction,
latent variable models

\paragraph{Relevance to BAA}
As information networks get larger and more complex,
acquiring explicit supervision for the training of information extraction systems
becomes extremely expensive.
Information networks are also dynamic;
over time one method for representing information may become inadequate.
Currently, both the representation of the information in a knowledge base
as well as the extraction process itself must be hand-designed.
In this proposal we present a framework towards automating the
training of information extraction systems with minimal supervision.

\paragraph{Introduction}
% Define generative model?
Natural language processing contains two separate but
closely related subfields: natural language understanding (NLU) and natural language
generation (NLG).
Recent approaches to information extraction use only the NLU perspective
and frame extraction as a classification problem.
We argue that the two perspectives, NLU and NLG, are complementary
and capitalize on their duality by proposing a method to train a NLU system
without direct supervision.
%More specifically, we utilize the latent variable model framework to train a 
%neural network-based information extraction system and use as signal 
More specifically, we train an information extraction system by using
the performance of a deep generative model as signal.
By training in this fashion we obtain an information extraction system
that extracts facts that best explains the given text.

%\paragraph{Supervision in NLU}
Recent work using neural network-based systems cast information extraction
as a supervised problem
\citep{wiseman2017d2t,dossantos2015classification}.
Although approaches do incorporate structure into internal representations,
for example to integrate multiple sources of information \citep{weissenborn17},
the final output still uses strong supervision during training.
Finding strong supervision is a difficult task, and as datasets get larger
we must approach information extraction from a different perspective.
Latent variable models (LVMs) are one method for alleviating the need for supervision.
LVMs do not require manual labels, as they instead treat quantities of interest
as latent random variables and deal with them in a probabilistically principled 
fashion.
We will leverage recent advances in deep generative modeling in order to
scale and extend their model while still training without explicit supervision.

%\paragraph{Recent advances in NLG}
Neural models for language generation have seen much progress in recent years,
with state of the art performance in both language modeling and translation \citep{?} (MoS + Transformer).
By integrating the powerful language modeling capabilities of neural networks with
the inductive biases of graphical models through LVMs,
we can leverage the LVM framework to derive a principled method
for training an information extraction system with less supervision.
Namely, we turn to an efficient technique for training hard attention 
that relies on variational inference \citep{deng2018attn}.
This technique is applicable to sequential LVMs such as hidden Markov models 
and hidden semi-Markov models (HSMMs).
HSMMs have been applied to small scale datasets for text generation \citep{wiseman2018template}
as well as without integrating neural networks \citep{liang2009semalign}.
We will scale sequential LVMs to large datasets through variational inference and use
the signal from our generative model in order to train an information extraction system.

%Modern neural systems have demonstrated state-of-the-art performance on short-form
%text generation tasks, but the performance seemingly fails to generalize to long-form generation.
%D2T is a task that isolates performance on long-form generation by disentangling 
%generation quality from input representation.
\paragraph{Background}
We consider datasets consisting of aligned data and text
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single datum and text as $\br,\by$, omitting the superscript.
Each datum $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
The datum $\br$ is a flattened representation of an information network.
Whereas an information network may also be represented as a hypergraph,
$\br$ is a list of relations or records.
We refer collectively to all the entities, types, and values in a given datum $\br$ as
$\be,\bt,\bv$ respectively.
Each text $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens.

Let random variables $\bz$ be unobserved or latent, $\by$ observed, and $\bx$ taken as conditioning
and thus not modelled.
For information extraction we are interested in distributions $p(\bz\mid\by,\bx)$,
where $\bz$ and $\bx$ may correspond to various quantities depending on the task
but $\by$ is always the text.

As a concrete example, we use the Rotowire dataset \citep{wiseman2017d2t}.
Rotowire contains summaries of basketball games $\by$ aligned with the respective
box scores $\br$ of those games.
Consider a datum that consists of a single record,
$\br = \set{(e_1 = \textrm{Jeremy Lin}, t_1 = \textrm{POINTS}, v_1 = 19)}$,
and a simple statement $\by = $``Jeremy Lin scored 19 points''.
In a simple case, the process of information extraction may be to infer any
subset of $\br$, which in this case will be our latent $\bz$, given its complement in $\br, \bx = \bz^C$,
as well as the text $\by$.
For example, we may want to infer the type of the relation $t$ given 
the entity Jeremy Lin, the value 19, as well as the text $\by$.
In this case, we would have $\bz = \set{t_1}, \bx = \set{e_1,v_1}$.
In an alternative task, we may want to infer the value $v_1$ 
as well as the type $t_1$ given $\by$ and $e_1$, therefore $\bz = \set{t_1,v_1}, \bx=\set{e_1}$.

Note that we are not constrained to setting $\bz$ to subsets of $\br$.
We also consider the case where $\bz$ includes alignments from individual words $y_t$
to records $r_i$. We denote the alignments $\bc = \set{c_1,\ldots,c_T}$,
where each $c_t$ is associated with $y_t$ and selects a record $r_i$ such that $c_t = i$.

\paragraph{Proposal}
We propose to demonstrate the efficacy of the LVM framework in the
weakly supervised information extraction setting.
We do so by estimating the distribution $p(\bz\mid\by,\bx)$,
where $\bz = \set{\bc, \bv}$ .

JUNK PAST THIS POINT, DO NOT READ.

\paragraph{Old Prop}
Our goal is to maximize the \underline{coverage} of the information extraction system,
which we define as the number of words contained within segments of text that are correctly aligned to data,
while minimizing the amount of supervision needed.
We propose to learn an information extraction (IE) system as the approximate posterior distribution over 
alignments from segments of text to their generating data.
We also propose to view the given data as incomplete
and learn boolean-valued functions of the data as a step
towards representation learning.

\paragraph{Problem Definition}
\begin{comment}
\item Define notation
\item Define generation, mention sam and puduppully work.
\item Define extraction and its subtasks, mention regina and percy's work.
\item TODO: maybe combine align and values? Although I prefer to keep them separate
since values will probably turn into fill in the away team's values given home team's
\end{comment}
Rotowire consists of aligned box score data and basketball game summaries
$\set{(\br^{(1)}, \by^{(1)}),(\br^{(2)},\by^{(2)})\ldots}$.
For brevity, we refer to a single data and summary as $\br,\by$, omitting the superscript.
Each data $\br = \set{r_1,\ldots,r_N}$ is a set of $N$ records, each of which has
an entity, type, and value $r_i = (e_i, t_i, v_i)$.
We refer collectively to all the entities, types, and values in a given data $\br$ as
$\be,\bt,\bv$ respectively.
Each summary $\by = \set{y_1,\ldots,y_T}$ is a sequence of tokens that makes up the
text of the game description.

For generation, the goal is to learn a conditional model $p(\by\mid\br)$ of the text given the data.
This is simple to evaluate, as we can use the log-likelihood of a given summary under our model
as a measure of performance.
In \citet{wiseman2017d2t} the model takes the form of a conditional language model
that can copy values directly from records in $\br$.
Subsequent work in \citet{puduppully2018contentselection} decomposed the distribution 
$p(\by\mid\br) = \sum_{\bc}p(\by\mid\bc)p(\bc\mid\br)$
by introducing a content plan $\bc$, which is a sequence of records drawn from $\br$.
This was also previously implemented in prior work \citep{liang2009semalign},
which modelled the text generation process through a hierarchical hidden semi-Markov model.
% Definitely need a figure here

We divide the information extraction task into three subtasks.
Before outlining the tasks, we propose two measures of performance through which
to evaluate an unsupervised information extraction system.
The first is how well the information extracted from a summary allows a 
generative model to reconstruct the summary measured by the likelihood
of the summary given the extracted information.
We refer to this as \underline{reconstruction}.
The second is \underline{coverage}, 
which we define as the number of words contained in segments
that are aligned to a record in $\br$.
(Do I need to argue why these are useful?
And also how the tasks aim at increasing them by weakening assumptions
or constraining model flexibility compared to previous work)

The first task (\textsc{align}) is to align segments of text to the records that generated them.
This is similar to learning a content plan $\bc\mid\br$ as in \citet{puduppully2018contentselection},
however we are interested in the \textbf{posterior} distribution $p(\bc\mid\br,\by)$ of
the content plan $\bc$ after observing the text $\by$.
\citet{liang2009semalign} utilize the fact that they define a model in which posterior inference is tractable,
however tractability does not hold once the latent distribution becomes autoregressive.
% Elaborate
In \citet{wiseman2017d2t} and subsequently in \citet{puduppully2018contentselection} this was accomplished by 
separately training a classifier to predict the type $t$ of an entity $e$ and value $v$ pair within a sentence.
The entity and value are extracted heuristically by checking exact string matches within $\be$ and $\bv$,
and the supervision over $t$ is obtained through the following function \citep{wiseman2017d2t}:
$\text{findType}(\hat{e},\hat{v}) = \set{ r.t :x\in\br, r.e = \hat{e}, x.r = \hat{v}}$.
However, this limits alignments exclusively to entities and values explicitly in $\br$.
We would like to align whole segments of text in order to increase the coverage
of our information extraction system.


The second task (\textsc{values}) is to reconstruct values $v$ in the table $\br$.
This is implemented on top of task (\textsc{align}).
In particular, we want to learn $p(\bv \mid \by,\bc,\be,\bt)$,
the distribution over all values given the summary, the content plan, all entities, and all types.

The third task (\textsc{functions}) is the most ambitious.
In order to demonstrate the flexibility of the framework,
we propose a method to further learn functions of $\br$ in an unsupervised manner.
(TODO)

\paragraph{Model}
We begin by defining a model for (\textsc{align})
and proceed to (\textsc{values}) and (\textsc{functions}).

% Generative model
Our generative model factors into the
likelihood and prior: $p(\by,\bc\mid\br)=p(\by\mid\bc)p(\bc\mid\br)$.
Our likelihood $p(\by\mid\bc)$ is given by a conditional neural language model
with a copy mechanism as in \citet{gulcehre2016cc,wiseman2017d2t}
in addition to monotonic attention \citep{yu2016ssnt,wiseman2018template}.
The prior $p(\bc\mid\br)$ is an autoregressive model over records
parameterized with an LSTM.
As we are primarily interested in posterior inference, the performance of the prior is
not the most important aspect of the model.
In fact, we will see in the next section that the prior serves to regularize
our approximate posterior.
(This is the simplest baseline aside from HSMM, can include 
$p(\by,\bc\mid\br)=\prod_tp(y_t\mid\by_{<t},c_t)p(c_t\mid\bc_{<t},\by_{<t},\br)$
if necessary)

% IE model
Our initial IE model for (\textsc{align}) is given by $q(\bc\mid\by,\br)$,
which includes both a segmentation of the summary as well as the alignments.
Note that we denote the distribution using $q$ since under the generative model
$p(\bc\mid\by,\br)$ is well-defined as the posterior distribution of alignments
given a summary and records.
Initially we parameterize the approximate posterior
$q(\bc\mid\by,\br)=\prod_tq(c_i\mid\by,\br)$
as a fully factored distribution over alignments.
(Structured attention for pairwise potentials later,
since only has node potentials for now.
This is motivated by \underline{coverage})
Each $q(c_i\mid\by,\br)$ is parameterized by the output
of a BLSTM run at the sentence level.
(TODO: values, functions)

\paragraph{Learning and Inference}
\begin{comment}
\item Question: Do I need to motivate approximate inference?
I could also argue that rather than computing the posterior exactly,
at test time using the approximation directly can be more efficient,
especially if it is fully factored
\item Need to figure out if this is the right place for this
\item Describe how to incorporate the weak supervision through posterior constraints
\end{comment}
A latent variable model $p(\by,\bz\mid\bx)$ models an
observed $\by$ as well as an unobserved $\bz$ conditioned on $\bx$.
When fitting such a model, we would like to maximize the evidence
$p(\by\mid\bx)=\sum_\bz p(\by,\bz\mid\bx)$ which marginalizes over
the latent $\bz$.
Depending on the structure of $p(\by,\bz\mid\bx)$,
the marginalization procedure may be intractable to perform exactly.
For example, this is the case with an autoregressive model for the latent
variable $p(z_t\mid \bz_{<t})$, where variable elimination's runtime would be
exponential in the length $|\bz|$.
This also precludes tractable posterior inference, i.e. $p(\bz\mid\by,\bx)$,
since by Bayes' Rule we have $p(\bz\mid\by,\bx)=p(\by,\bz\mid\bx)/p(\by\mid\bx)$
which requires evaluating the intractable sum in the evidence $p(\by\mid\bx)$.
Therefore we resort to learning an approximation of the posterior through the variational principle:
the best approximation within a family of distributions is the one with minimal KL-divergence
to the model's posterior.
The KL between the approximate posterior and true posterior is still intractable to minimize exactly,
so we instead maximize the evidence lower bound ($\mathcal{L}_q$), which is the evidence minus the posterior KL:
\begin{linenomath*}
\begin{align}
\mathcal{L}_q
&= \underbrace{\log p(\by\mid\bx)}_{\textrm{Evidence}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx)||p(\bz\mid\by,\bx)]}_{\textrm{Posterior KL}}\\
&= \underbrace{\Es{\bz\sim q(\bz\mid\by,\bx)}{\log p(\by\mid\bz,\bx)}}_{\textrm{Reconstruction}}
- \underbrace{D_{KL}[q(\bz\mid\by,\bx) || p(\bz\mid\bx)]}_{\textrm{Prior KL}}.
\end{align}
\end{linenomath*}
Were it not for local extrema,
maximizing this quantity would maximize the evidence and minimize the posterior KL jointly.
Notice that in (2), the objective we use for training both the generative model and IE system,
all expectations are taken with respect to the IE system or approximate posterior $q(\by\mid\bz,\bx)$.
(TODO: training procedure via REINFORCE + control variate, posterior constraints for incorporating 
information from findType function)

For task (\textsc{align}), we have the fully observed summary $\by$,
the unobserved content plan $\bz=\bc$, and all records as conditioning $\bx=\br$.
For task (\textsc{values}), we again have the observed summary $\by$,
but we pretend the values are unobserved $\bz=\set{\bc,\bv}$, and
use the rest of the records as conditioning $\bx=\set{\be,\bt}$.
(TODO: functions)

\newpage

\section*{Outline}
\begin{enumerate}
\item Introduction
    \begin{enumerate}
    \item Relevance to BAA
        \begin{enumerate}
        \item What are information networks? Characterized by large graphs and voluminous data.
        \item Representation is key
        \item Supervision is expensive.
        \item Information networks are also dynamic;
            over time one method for representing information may become inadequate.
        \item Currently, both the representation of the information in a knowledge base
            as well as the extraction process itself must be hand-designed.
        \item In this proposal we present a framework towards automating the
            training of information extraction systems with minimal supervision.
        \end{enumerate}
    \item Decomposition of NLP into NLU and NLG
        \begin{enumerate}
        \item Two separate but closely related subfields: natural language understanding (NLU) and natural language
            generation (NLG).
        \item Recent approaches to information extraction use only the NLU perspective
            and frame extraction as a classification problem.
        \item We argue that the two tasks are complementary,
            and capitalize on their duality by proposing a method to train a NLU system
            without direct supervision.
        \end{enumerate}
    \item Supervision in NLU
        \begin{enumerate}
        \item Recent work using neural network-based systems cast information extraction
        as a supervised problem
        \citep{wiseman2017d2t,dossantos2015classification}.
        \item Even with structured representations \citep{weissenborn17}, the final output is still supervised.
        \item Again, supervision does not scale.
        \item Maybe dynamism of information networks? Don't have an argument on hand, though.
        \item Latent variable models (LVMs) are one method for alleviating the need for supervision.
        \item LVMs do not require manual labels, as they instead treat quantities of interest
            as latent random variables and deal with them in a probabilistically principled 
            fashion.
        \end{enumerate}
    \item Recent advances in NLG
        \begin{enumerate}
        \item Deep generative models, namely LVMs with neural network components,
            have integrated the flexibility of neural networks with the
            inductive biases of graphical models.
        \item Most importantly, efficient techniques for training hard attention 
            that rely on variational inference \citep{deng2018attn}.
            This technique is applicable to sequential LVMs such as hidden Markov models 
            and hidden semi-Markov models (HSMMs).
        \item HSMMs have been applied to small scale datasets for text generation \citep{wiseman2018template}
            as well as without integrating neural networks \citep{liang2009semalign}.
        \end{enumerate}
    \end{enumerate}
\item Background
    \begin{enumerate}
    \item Formal notation for elements of the dataset
    \item Define the distribution we would like to learn: $p(z\mid y, x)$.
        $z$ and $x$ are placeholders and will change, but $y$ is always the text.
    \item Link to rotowire example
        (argument is that ACE is made up of ontonotes-like sentences, so all short-form)
    \end{enumerate}
\item Proposal
    \begin{enumerate}
    \item Define IE tasks as the distributions we would like to learn
        \begin{enumerate}
        \item Align: $p(c\mid\by,\br)$
        \item Values: $p(c,\bv\mid\by,\be,\bt)$
        \item ??: $p()$
        \end{enumerate}
    \item Define generative model: h-HSMM as in \citep{liang2009semalign},
        but with neural components as in \citep{wiseman2018template}
    \item Training and Inference
        \begin{enumerate}
        \item Maximize ELBO
        \end{enumerate}
    \item Experiments
        \begin{enumerate}
        \item 
        \end{enumerate}
    \item Conclusion
        \begin{enumerate}
        \item 
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

