\documentclass[11pt]{article}

\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{lipsum}

% ToC
\usepackage{blindtext} 
\usepackage[linktocpage]{hyperref}
\usepackage{bookmark}
\usepackage[compact]{titlesec}

% bib
%\usepackage[round]{natbib}
\usepackage[square,sort,numbers]{natbib}

% Math Imports
\usepackage{amsmath, amssymb, bm, fancyhdr, sectsty, dsfont, mathtools}

% Tikz
\usepackage{tikz}
\usetikzlibrary{bayesnet}
\usetikzlibrary{arrows}

\usepackage{wrapfig}
\usepackage{comment}
\usepackage{subcaption}
\usepackage{cleveref}

% Symbols
\newcommand\ind{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\set[1]{\left\{#1\right\}}

\newcommand\RNN{\mathrm{RNN}}
\newcommand\MLP{\mathrm{MLP}}
\newcommand\enc{\mathrm{enc}}
\newcommand\softmax{\mathrm{softmax}}

% Distributions
\newcommand{\Cat}{\mathrm{Cat}}
\newcommand\Expo{\mathrm{Expo}}
\newcommand\Bern{\mathrm{Bern}}
\newcommand\Pois{\mathrm{Pois}}
\newcommand\Bin{\mathrm{Bin}}
\newcommand\Unif{\mathrm{Unif}}
\newcommand\Betad{\mathrm{Beta}}
\newcommand\Gammad{\mathrm{Gamma}}
\newcommand\Geom{\mathrm{Geom}}
\newcommand\Logd{\mathrm{Logistic}}

\newcommand\E[1]{\mathbb{E}\left[#1\right]}
\newcommand\Es[2]{\mathbb{E}_{#1}\left[#2\right]}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Cor}{\mathrm{Cor}}

% Bold stuff
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bc}{\mathbf{c}}
\newcommand{\bd}{\mathbf{d}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\br}{\mathbf{r}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bz}{\mathbf{z}}

% mathcal stuff
\newcommand{\mcD}{\mathcal{D}}

% math blackboard bold stuff
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}


\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}

\titlespacing{\paragraph}{0pt}{*0}{*0}
%\setlength{\parskip}{-5mm plus1mm minus1mm}

\usepackage{fancyhdr}
\pagestyle{fancy}


\begin{document}
\lhead{Justin Chiu}
\chead{2018 NSF GRFP}
\rhead{Research Proposal}

\begin{center}
\textbf{Latent Variable Models for Data to Text Generation}
\end{center}

\paragraph{Keywords}
latent variable models, conditional text generation, data to text

\paragraph{Background}
Natural language processing (NLP) can largely be decomposed into two separate but
tightly related tasks: natural language understanding and natural language
generation (NLG).
In this proposal we focus primarily on generation, and in particular
the generation of text that describes structured data.
We refer to this specific task as Data-to-Text (D2T).
The assumption that the text describes data is central to the task as it allows
for practitioners to define heuristics for evaluating a summary's fidelity to the
underlying data.

\paragraph{Why is D2T important?}
\begin{itemize}
\item Long-form generation but with structured input: a compromise between
short-form with unstructured inputs and long-form with unstructured inputs
\item Easily evaluable benchmark due to limited use of external knowledge
\end{itemize}
The D2T task's importance is two-fold.
First, D2T serves as a useful benchmark for conditional generation.
D2T exists as a compromise between short-form generative tasks such as sentence-level
translation and unstructured long-form tasks such as generating Wikipedia articles
from internet resources.
This is because D2T is a long-form generation task but uses structured data
as input instead of free-form text.
Secondly, the existence of structured data represents a simplifying assumption
which makes D2T easier than other long-form tasks but also, more importantly,
amenable to automatic evaluation.
Given a list of records and the assumption that in a data-text pair the text's purpose
is to describe the data,
it is straightforward to obtain possible alignments between the text and records for evaluation.
However, the alignments can also be used for much more than evaluation; they can be used for
obtaining weak supervision \citep{puduppully2018contentselection}. %in a latent variable model.
This insight motivates our proposal and will be elaborated on shortly.

\paragraph{What are the desiderata of a text generation system?}
\begin{enumerate}
% This is an iffy statement
\item Readability: Fluency and grammaticality.
Prior work has shown that models with fully connected observations models
in the form of a neural network have this quality \citep{wiseman2017d2t} (as measured by BLEU).
\item Informational adequacy: This is decomposed into content selection and planning.
We can represent this directly in a model with a language model over
records \citep{puduppully2018contentselection}.
\item Fidelity to conditioning: We can decompose factual errors into two types.
The first is misalignment of records and the second is ignoring the record
completely and hallucinating values with a language model.
\citep{puduppully2018contentselection} attempt to address this by
attending over a content plan.
However, there are additional constraints that we know to be true
but are not modelled in their model:
namely that the alignment between text and record should be monotonic.
% Should I address this in more detail?
% Hypothesize that constraining the model results in sharper alignments
% and less noise, increasing fidelity to records
\item Controllability: The ability to impose constraints and intervene
is a strict superset of interpretability.
We see this materialize in \citep{puduppully2018contentselection}
as with their model they can manipulate the content plan
and control the generated summary.
\end{enumerate}

\paragraph{Current Trends}
Early work on D2T focused on the use of templates and grammars, 
which generally resulted in generations with little stylistic variation.
% Do I need to qualify this claim and say why?
After neural language models demonstrated state of the art performance,
they were then transferred to the conditional language modeling setting,
i.e. D2T and more broadly NLG,
where they supplanted template-based methods almost completely.
The result of this using neural language models was an improvement in readability
at the cost of fidelity as well as controllability.
%However, by relying largely on the language modeling capabilities of neural networks,
%practitioners have eschewed modularity of the model for ease of inference.
% Do I need to qualify this as well?
%\citep{angeli2010d2t}, \citep{liang2009semalign}, \citep{sauper2009wiki}

\paragraph{Research Question}
%We can train end-to-end modular systems,
%so how much modularity and supervision can we provide and does it help the model?
As research on D2T has embraced neural systems and end-to-end training resulting
in very unstructured models, what is the benefit of imbuing more structure, namely in the
form of a hierarchical segmental model?
%As we add more complexity to our graphical model, what is the benefit of a latent variable model
%versus a soft approximation?

\paragraph{Argument for LVM}
\begin{enumerate}
\item The fully neural approach in \citep{puduppully2018contentselection} takes the output of
an information extraction system as the ground truth and trains a content planning model with
that assumption.
Any mistakes in the information extraction system will be replicated in the final model.
%In addition, the information extraction is trained via a heuristic alignment
%procedure that aligns words that appear in the database to records containing those words. 
%However, we argue that this particular procedure results in a compounding of mistakes.
% should use more precise notation here
Thus we argue that training the information extraction system by incorporating it
into an approximate posterior will improve performance as measured by
the likelihood of the data under our model.
\item As demonstrated in \citep{liang2009semalign},
a structured LVM results in qualitatively better segmentations than an unstructured one.
\item Convincing templates from prior work \citep{wiseman2018template} indicate that the separation of
style and content is plausible.
\end{enumerate}

\paragraph{Data}
% Example of D2T, should i present this early or later
The dataset we focus on is the Rotowire dataset \citep{wiseman2017d2t},
where a summary of a basketball game is modelled conditioned on the box score
associated with that game.
This is the most recent D2T dataset that has been developed.
The box score consists of a list of records associating an entity with a value.
Each record has a type which denotes the relationship between the entity and value
contained in the record,
for example: (type = POINTS, entity = Jeremy Lin, value = 19).

\begin{comment}
A vein in the recent work on integrating neural network systems and latent variable models
has aimed to separate content from style.
Separate style and content, where content will come from for controllable generation.
\end{comment}
%Can we also model intention in a similar way?
%Hierarchical planning, multiple levels
%cite fair thing with dialogue planning? 

\begin{comment}
\paragraph{Related Work}
In the task of generating basketball summaries from game statistics,
\citet{puduppully2018contentselection} learn a model of content progression
by learning how to order records from the table of statistics.
\citep{puduppully2018contentselection,wiseman2018template}
Separate content from style
\end{comment}

\begin{wrapfigure}[14]{r}{0.45\textwidth}
\centering
\begin{comment}
\begin{subfigure}[]{0.4\textwidth}
\centering
\begin{tikzpicture}
\node(x)[obs]{$\bx$};
\node(r)[latent, right =of x]{$\br$};
\node(y)[latent, right =of r]{$\by$};
\edge {x} {r};
\edge {r} {y};
\end{tikzpicture}
\caption{}
\end{subfigure}
\end{comment}
\begin{subfigure}[]{0.4\textwidth}
\centering
\begin{tikzpicture}
\node(x)[obs]{$\bx$};
\node(s)[latent, right =of x]{$\bs$};
\node(r)[latent, right =of s]{$\br$};
\node(y)[latent, right =of r]{$\by$};
\edge {x} {s};
\edge [bend right=30] {x} {r};
\edge {s} {r};
\edge [bend right=30] {s} {y};
\edge {r} {y};
\end{tikzpicture}
\label{tikz:simple}
\caption{}
\end{subfigure}

\begin{subfigure}[]{0.4\textwidth}
\centering
\begin{tikzpicture}
\node(s0)[latent]{$s_0$};
\node(r0)[latent, below =of s0]{$r_0$};
\node(r0d)[latent, below right =4mm and 3mm of r0]{$r_0^D$};
\node(y0)[latent, below =of r0]{$y_0$};
\node(s1)[latent, right =of s0]{$s_1$};
\node(r1)[latent, below =of s1]{$r_1$};
\node(r1d)[latent, below right =4mm and 3mm of r1]{$r_1^D$};
\node(y1)[latent, below =of r1]{$y_1$};
\node(s2)[latent, right =of s1]{$s_2$};
\node(r2)[latent, below =of s2]{$r_2$};
\node(r2d)[latent, below right =4mm and 3mm of r2]{$r_2^D$};
\node(y2)[latent, below =of r2]{$y_2$};

\edge {s0} {r0};
\edge [bend right=35] {s0} {y0};
\edge {r0} {y0};
\edge {r0} {r0d};

\edge {s1} {r1};
\edge [bend right=35] {s1} {y1};
\edge {r1} {y1};
\edge {r1} {r1d};

\edge {s2} {r2};
\edge [bend right=35] {s2} {y2};
\edge {r2} {y2};
\edge {r2} {r2d};

\edge {s0} {s1};
\edge {r0} {r1};
\edge {y0} {y1};
\edge {r0d} {r1};

\edge {r0d} {r1d};
\edge [bend left=15] {y0} {s1};

\edge {s1} {s2};
\edge {r1} {r2};
\edge [bend right=25] {y0} {y2};
\edge {y1} {y2};
\edge {r1d} {r2};

\edge {r1d} {r2d};
\edge [bend left=15] {y1} {s2};


\end{tikzpicture}
\label{tikz:full}
\caption{}
\end{subfigure}
\label{fig:pgm}
\caption{
(a) is a simplified graphical representation of the model
that ignores the temporal dependencies.
$\bx$ is the input data, $\bs$ the style, $\br$ the content,
and $\by$ the words of the summary.
(b) is the full graphical model.
We leave out conditioning on $\bx$ for brevity.
The new variables $r_i^D$ represent the duration a record is used.
}
\end{wrapfigure}


\paragraph{Approach and Methods}
% Proposal? Add more structure / modularity, principled inference...?
% Thesis: structure will help with controllability and LVM formulation
% with generalization as it presents a different inductive bias?
We would like to unify works from several directions:
\begin{enumerate}
\item Separating style and content through templates \citep{sauper2009wiki,wiseman2018template}
\item Content planning and incorporating weak supervision \citep{puduppully2018contentselection}
\item Efficient training methods for discrete LVMs \citep{deng2018vattn}
\end{enumerate}
% Elaborate on each of those points
We propose to model the conditional distribution of a summary given data as a 
hierarchical hidden semi-markov model (HSMM), as in \citep{liang2009semalign}.
Ignoring the temporal aspect of the model, we define a simplified model
with the following parts:
\begin{enumerate}
\item The given structured data $\bx$ 
\item Each sentence is generated using a style $s_i$
\item The content in each sentence $\br_i$ is chosen from $\bx$
after choosing a style $s_i$
\item The words in the summary $y_i$ are chosen conditioning on
the current content from $r_i$ as well as the style $c_i$
\end{enumerate}
\begin{comment}
We also present a more detailed model:
% Caveat: we don't want r_t to depend on r_{t-1} if starting a new sentence.
\begin{enumerate}
\item The style features $s_t\mid y_{t-1},s_{t-1},\bx\sim\Cat()$
\item The record choices $r_t\mid r_{t-1}^D,r_{t-1},s_t,\bx\sim\Cat()$
\item The record duration is given by 
$r_t^D\mid r_{t-1}^D=0,r_t\sim\Unif(1,\ldots,L)$
where $L$ is the max segment length and $r_t^D\mid r_{t-1}^D=x = x-1$

\item The words in the summary $y_t\mid y_{<t},r_t,s_t\sim\Cat()$
\end{enumerate}
\end{comment}
See Figure~\ref{fig:pgm} for a simplified graphical depiction of the model.
We include a fully-specified slice of the time-series model as well,
but do not elaborate on it.

% too expensive to align to records since there may be many of them
As defined, the model would be very expensive to train via maximum marginal likelihood
training. We therefore propose to use the method proposed in \citep{deng2018vattn}
in order to make training tractable: namely define a variational approximation to the
posterior and use REINFORCE.
We also propose to utilize the posterior regularization framework in order to incorporate 
intermediate weak supervision in the form of record alignments.

% Should I explain why I propose a LVM versus a fully discriminative model?
% 
\paragraph{Intellectual Merit}
We hope to demonstrate that defining a modular model 
\paragraph{Broader Impact}

\bibliographystyle{plainnat}
\bibliography{w}

\end{document}

